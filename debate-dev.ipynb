{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Import AutoGen components\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiplomaticDebateSystem:\n",
    "    def __init__(self, openai_api_key, gemini_api_key, vllm_endpoint=\"http://localhost:8000/generate\"):\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.gemini_api_key = gemini_api_key\n",
    "        self.vllm_endpoint = vllm_endpoint\n",
    "        self.system_prompts = self._create_system_prompts()\n",
    "        self.agents = {}\n",
    "        self.debate_logs = []\n",
    "        \n",
    "    def _create_system_prompts(self):\n",
    "        \"\"\"Create system prompts for each country based on the research design\"\"\"\n",
    "        india_prompt = \"\"\"\n",
    "        **Role**: You are the Chief Negotiator for India at the WTO TRIPS Council.\n",
    "        **Directives**:\n",
    "        1. Base arguments on:\n",
    "            - The 2020 India/South Africa waiver proposal (IP/C/W/669)\n",
    "            - Sections 83/84/92A of India Patents Act 1970\n",
    "            - 2022 WHO vaccine equity resolution WHA75.15\n",
    "        2. Rhetorical Constraints:\n",
    "            - Use developing-country coalition building strategies\n",
    "            - Reference Doha Declaration Paragraph 4 health safeguards\n",
    "            - Reject analogies to non-pandemic IP regimes\n",
    "        3. Prohibited Concepts:\n",
    "            - Voluntary licensing as sufficient solution\n",
    "            - Pharma R&D cost recovery arguments\n",
    "            - \"Theft\" framing of compulsory licenses\n",
    "        \n",
    "        You must maintain India's position on TRIPS flexibility for COVID vaccines, emphasizing vaccine equity requires Art.31bis expansion + tech transfer.\n",
    "        \"\"\"\n",
    "        \n",
    "        switzerland_prompt = \"\"\"\n",
    "        **Role**: You represent Switzerland's Federal Council in WTO negotiations.\n",
    "        **Directives**:\n",
    "        1. Legal Foundation:\n",
    "            - Art. 29 Swiss Constitution (IP protection mandate)\n",
    "            - 2022 SECO FTA Strategy Paper (Section 4.3)\n",
    "            - WTO Dispute DS363 compliance records\n",
    "        2. Argumentation Rules:\n",
    "            - Emphasize 70% COVID vaccine dose donations via COVAX\n",
    "            - Cite 2021 Pharma R&D investment ($2.3B domestic)\n",
    "            - Require \"concrete evidence\" for each waiver request\n",
    "        3. Prohibited Inferences:\n",
    "            - Equating patent pools with IP confiscation\n",
    "            - Historical colonial trade pattern analogies\n",
    "            - TRIPS Agreement non-compliance allegations\n",
    "        \n",
    "        You must maintain Switzerland's position that existing TRIPS flexibilities are sufficient when properly implemented.\n",
    "        \"\"\"\n",
    "        \n",
    "        moderator_prompt = \"\"\"\n",
    "        You are a neutral moderator for WTO TRIPS Council debates. Your role is to:\n",
    "        1. Ensure fair speaking time for all parties\n",
    "        2. Summarize key positions objectively\n",
    "        3. Identify potential areas of compromise\n",
    "        4. Draft balanced resolutions that reflect all perspectives\n",
    "        \n",
    "        Base your moderation on:\n",
    "        - WTO Dispute Settlement Understanding (DSU)\n",
    "        - Vienna Convention on Treaty Interpretation\n",
    "        - 2021 UNCTAD Trade Law Reports\n",
    "        \n",
    "        Avoid showing preference for any country's position.\n",
    "        \"\"\"\n",
    "        \n",
    "        judge_prompt = \"\"\"\n",
    "        You are an impartial judge evaluating a diplomatic debate between India and Switzerland on the topic of pharmaceutical patent waivers for COVID-19 vaccines.\n",
    "        \n",
    "        Evaluate the debate on these criteria:\n",
    "        1. Positional Consistency: How well did each country maintain alignment with their assigned nation's historical policies?\n",
    "        2. Argument Quality: Rate arguments on persuasiveness and factual grounding (1-10 scale)\n",
    "        3. Resolution Influence: How much did each country's position influence the final resolution?\n",
    "        \n",
    "        Provide a detailed analysis with specific examples from the debate. Format your evaluation as JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"india\": india_prompt,\n",
    "            \"switzerland\": switzerland_prompt,\n",
    "            \"moderator\": moderator_prompt,\n",
    "            \"judge\": judge_prompt\n",
    "        }\n",
    "    \n",
    "    async def setup_agents(self):\n",
    "        \"\"\"Initialize all agents for the debate\"\"\"\n",
    "        # Create LLM clients\n",
    "        vllm_client = OpenAI(\n",
    "            api_key=\"EMPTY\",  # vLLM doesn't require a real API key\n",
    "            base_url=\"http://localhost:8000/v1\"  # Your vLLM server endpoint\n",
    "        )\n",
    "        \n",
    "        # gemini_client = OpenAIChatCompletionClient(\n",
    "        #     model=\"gemini-2.0-flash\",\n",
    "        #     api_key=self.gemini_api_key,\n",
    "        #     base_url=\"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "        # )\n",
    "        \n",
    "        # Create agents\n",
    "        self.agents[\"india\"] = AssistantAgent(\n",
    "            name=\"India\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"india\"]\n",
    "        )\n",
    "        \n",
    "        self.agents[\"switzerland\"] = AssistantAgent(\n",
    "            name=\"Switzerland\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"switzerland\"]\n",
    "        )\n",
    "        \n",
    "        self.agents[\"moderator\"] = AssistantAgent(\n",
    "            name=\"Moderator\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"moderator\"]\n",
    "        )\n",
    "        \n",
    "        self.agents[\"judge\"] = AssistantAgent(\n",
    "            name=\"Judge\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"judge\"]\n",
    "        )\n",
    "        \n",
    "        # # Create user proxy for initiating the debate\n",
    "        # self.agents[\"user\"] = UserProxyAgent(\n",
    "        #     name=\"User\",\n",
    "        #     human_input_mode=\"NEVER\"\n",
    "        # )\n",
    "    \n",
    "    async def run_debate(self, topic=\"Pharmaceutical Patent Waivers for COVID-19 Vaccines\", rounds=3):\n",
    "        \"\"\"Run a complete debate session\"\"\"\n",
    "        # Define termination condition\n",
    "        termination = TextMentionTermination(\"FINAL RESOLUTION\")\n",
    "        \n",
    "        # Create debate team (without judge)\n",
    "        debate_team = RoundRobinGroupChat(\n",
    "            [self.agents[\"moderator\"], self.agents[\"india\"], self.agents[\"switzerland\"]],\n",
    "            termination_condition=termination\n",
    "        )\n",
    "        \n",
    "        # Initialize the debate\n",
    "        opening_prompt = f\"\"\"\n",
    "        Welcome to this diplomatic debate on {topic}. \n",
    "        \n",
    "        Moderator, please introduce the topic and invite opening statements from India and Switzerland.\n",
    "        \n",
    "        After opening statements, facilitate {rounds} rounds of discussion, focusing on:\n",
    "        1. Core positions and legal justifications\n",
    "        2. Responses to opposing arguments\n",
    "        3. Potential compromise solutions\n",
    "        \n",
    "        Conclude by drafting a 'FINAL RESOLUTION' that reflects both positions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run the debate and collect messages\n",
    "        debate_stream = debate_team.run_stream(task=opening_prompt)\n",
    "        \n",
    "        # Collect all messages for evaluation\n",
    "        debate_log = []\n",
    "        async for result in debate_stream:\n",
    "            debate_log.append({\n",
    "                \"speaker\": result.source,  # Access attributes directly on the message\n",
    "                \"content\": result.content\n",
    "            })\n",
    "\n",
    "                # print(f\"{message.source}: {message.content[:100]}...\")  # Print preview\n",
    "        \n",
    "        self.debate_logs = debate_log\n",
    "        \n",
    "        # Extract the final resolution\n",
    "        final_resolution = next((msg[\"content\"] for msg in debate_log \n",
    "                               if \"FINAL RESOLUTION\" in msg[\"content\"]), \"No resolution found\")\n",
    "        \n",
    "        # Evaluate the debate using the judge\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Please evaluate the following diplomatic debate between India and Switzerland on {topic}.\n",
    "        \n",
    "        Debate transcript:\n",
    "        {json.dumps(debate_log, indent=2)}\n",
    "        \n",
    "        Final Resolution:\n",
    "        {final_resolution}\n",
    "        \n",
    "        Evaluate based on:\n",
    "        1. Positional Consistency (alignment with country's historical position)\n",
    "        2. Argument Quality (persuasiveness and factual grounding)\n",
    "        3. Resolution Influence (how much each country's position is reflected)\n",
    "        \n",
    "        Format your response as JSON with scores and justifications.\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluation = await self.agents[\"judge\"].generate_response(evaluation_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"debate_log\": debate_log,\n",
    "            \"final_resolution\": final_resolution,\n",
    "            \"evaluation\": evaluation\n",
    "        }\n",
    "    \n",
    "    async def save_results(self, results, filename=\"debate_results.json\"):\n",
    "        \"\"\"Save debate results to a file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def main():\n",
    "    # Replace with your actual API keys\n",
    "    openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "    gemini_api_key = os.environ['GEMINI_API_KEY']\n",
    "    \n",
    "    # Initialize the debate system\n",
    "    debate_system = DiplomaticDebateSystem(openai_api_key, gemini_api_key)\n",
    "    \n",
    "    # Setup agents\n",
    "    await debate_system.setup_agents()\n",
    "    \n",
    "    # Run the debate\n",
    "    results = await debate_system.run_debate()\n",
    "    \n",
    "    # Save results\n",
    "    await debate_system.save_results(results)\n",
    "    \n",
    "    # Print evaluation summary\n",
    "    print(\"\\nDebate Evaluation Summary:\")\n",
    "    print(results[\"evaluation\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TaskResult' object has no attribute 'source'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m debate_system.setup_agents()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Run the debate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m debate_system.run_debate()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m debate_system.save_results(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 160\u001b[39m, in \u001b[36mDiplomaticDebateSystem.run_debate\u001b[39m\u001b[34m(self, topic, rounds)\u001b[39m\n\u001b[32m    157\u001b[39m debate_log = []\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m debate_stream:\n\u001b[32m    159\u001b[39m     debate_log.append({\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspeaker\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m,  \u001b[38;5;66;03m# Access attributes directly on the message\u001b[39;00m\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: result.content\n\u001b[32m    162\u001b[39m     })\n\u001b[32m    164\u001b[39m         \u001b[38;5;66;03m# print(f\"{message.source}: {message.content[:100]}...\")  # Print preview\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28mself\u001b[39m.debate_logs = debate_log\n",
      "\u001b[31mAttributeError\u001b[39m: 'TaskResult' object has no attribute 'source'"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: \n",
      "        Welcome to this diplomatic debate on Pharmaceutical Patent Waivers for COVID-19 Vaccines. \n",
      "...\n",
      "user: \n",
      "        Welcome to this diplomatic debate on Pharmaceutical Patent Waivers for COVID-19 Vaccines. \n",
      "...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AssistantAgent' object has no attribute 'generate_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    248\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDebate Evaluation Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m     \u001b[38;5;28mprint\u001b[39m(results[\u001b[33m\"\u001b[39m\u001b[33mevaluation\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 242\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m debate_system.setup_agents()\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Run the debate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m debate_system.run_debate()\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m debate_system.save_results(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 216\u001b[39m, in \u001b[36mDiplomaticDebateSystem.run_debate\u001b[39m\u001b[34m(self, topic, rounds)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Evaluate the debate using the judge\u001b[39;00m\n\u001b[32m    199\u001b[39m evaluation_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[33mPlease evaluate the following diplomatic debate between India and Switzerland on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m    201\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m \u001b[33mFormat your response as JSON with scores and justifications.\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m evaluation = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjudge\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m(evaluation_prompt)\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    219\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdebate_log\u001b[39m\u001b[33m\"\u001b[39m: debate_log,\n\u001b[32m    220\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfinal_resolution\u001b[39m\u001b[33m\"\u001b[39m: final_resolution,\n\u001b[32m    221\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mevaluation\u001b[39m\u001b[33m\"\u001b[39m: evaluation\n\u001b[32m    222\u001b[39m }\n",
      "\u001b[31mAttributeError\u001b[39m: 'AssistantAgent' object has no attribute 'generate_response'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Import AutoGen components\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "import autogen_agentchat\n",
    "\n",
    "class DiplomaticDebateSystem:\n",
    "    def __init__(self, openai_api_key, gemini_api_key, vllm_endpoint=\"http://localhost:8000/v1\"):\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.gemini_api_key = gemini_api_key\n",
    "        self.vllm_endpoint = vllm_endpoint\n",
    "        self.system_prompts = self._create_system_prompts()\n",
    "        self.agents = {}\n",
    "        self.debate_logs = []\n",
    "        \n",
    "    def _create_system_prompts(self):\n",
    "        \"\"\"Create system prompts for each country based on the research design\"\"\"\n",
    "        india_prompt = \"\"\"\n",
    "        **Role**: You are the Chief Negotiator for India at the WTO TRIPS Council.\n",
    "        **Directives**:\n",
    "        1. Base arguments on:\n",
    "            - The 2020 India/South Africa waiver proposal (IP/C/W/669)\n",
    "            - Sections 83/84/92A of India Patents Act 1970\n",
    "            - 2022 WHO vaccine equity resolution WHA75.15\n",
    "        2. Rhetorical Constraints:\n",
    "            - Use developing-country coalition building strategies\n",
    "            - Reference Doha Declaration Paragraph 4 health safeguards\n",
    "            - Reject analogies to non-pandemic IP regimes\n",
    "        3. Prohibited Concepts:\n",
    "            - Voluntary licensing as sufficient solution\n",
    "            - Pharma R&D cost recovery arguments\n",
    "            - \"Theft\" framing of compulsory licenses\n",
    "        \n",
    "        You must maintain India's position on TRIPS flexibility for COVID vaccines, emphasizing vaccine equity requires Art.31bis expansion + tech transfer.\n",
    "        \"\"\"\n",
    "        \n",
    "        switzerland_prompt = \"\"\"\n",
    "        **Role**: You represent Switzerland's Federal Council in WTO negotiations.\n",
    "        **Directives**:\n",
    "        1. Legal Foundation:\n",
    "            - Art. 29 Swiss Constitution (IP protection mandate)\n",
    "            - 2022 SECO FTA Strategy Paper (Section 4.3)\n",
    "            - WTO Dispute DS363 compliance records\n",
    "        2. Argumentation Rules:\n",
    "            - Emphasize 70% COVID vaccine dose donations via COVAX\n",
    "            - Cite 2021 Pharma R&D investment ($2.3B domestic)\n",
    "            - Require \"concrete evidence\" for each waiver request\n",
    "        3. Prohibited Inferences:\n",
    "            - Equating patent pools with IP confiscation\n",
    "            - Historical colonial trade pattern analogies\n",
    "            - TRIPS Agreement non-compliance allegations\n",
    "        \n",
    "        You must maintain Switzerland's position that existing TRIPS flexibilities are sufficient when properly implemented.\n",
    "        \"\"\"\n",
    "        \n",
    "        moderator_prompt = \"\"\"\n",
    "        You are a neutral moderator for WTO TRIPS Council debates. Your role is to:\n",
    "        1. Ensure fair speaking time for all parties\n",
    "        2. Summarize key positions objectively\n",
    "        3. Identify potential areas of compromise\n",
    "        4. Draft balanced resolutions that reflect all perspectives\n",
    "        \n",
    "        Base your moderation on:\n",
    "        - WTO Dispute Settlement Understanding (DSU)\n",
    "        - Vienna Convention on Treaty Interpretation\n",
    "        - 2021 UNCTAD Trade Law Reports\n",
    "        \n",
    "        Avoid showing preference for any country's position.\n",
    "        \"\"\"\n",
    "        \n",
    "        judge_prompt = \"\"\"\n",
    "        You are an impartial judge evaluating a diplomatic debate between India and Switzerland on the topic of pharmaceutical patent waivers for COVID-19 vaccines.\n",
    "        \n",
    "        Evaluate the debate on these criteria:\n",
    "        1. Positional Consistency: How well did each country maintain alignment with their assigned nation's historical policies?\n",
    "        2. Argument Quality: Rate arguments on persuasiveness and factual grounding (1-10 scale)\n",
    "        3. Resolution Influence: How much did each country's position influence the final resolution?\n",
    "        \n",
    "        Provide a detailed analysis with specific examples from the debate. Format your evaluation as JSON.\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"india\": india_prompt,\n",
    "            \"switzerland\": switzerland_prompt,\n",
    "            \"moderator\": moderator_prompt,\n",
    "            \"judge\": judge_prompt\n",
    "        }\n",
    "    \n",
    "    async def setup_agents(self):\n",
    "        \"\"\"Initialize all agents for the debate\"\"\"\n",
    "        # Create LLM clients\n",
    "        vllm_client = OpenAI(\n",
    "            api_key=\"EMPTY\",  # vLLM doesn't require a real API key\n",
    "            base_url=\"http://localhost:8000/v1\"  # Your vLLM server endpoint\n",
    "        )\n",
    "        \n",
    "        # gemini_client = OpenAIChatCompletionClient(\n",
    "        #     model=\"gemini-1.5-flash-8b\",\n",
    "        #     api_key=self.gemini_api_key,\n",
    "        #     base_url=\"https://generativelanguage.googleapis.com/v1\",\n",
    "        #     model_info={\n",
    "        #         \"vision\": False,\n",
    "        #         \"function_calling\": True,\n",
    "        #         \"json_output\": False,\n",
    "        #         \"family\": \"gemini\",\n",
    "        #     }\n",
    "        # )\n",
    "        \n",
    "        # Create agents\n",
    "        self.agents[\"india\"] = AssistantAgent(\n",
    "            name=\"India\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"india\"]\n",
    "        )\n",
    "        \n",
    "        self.agents[\"switzerland\"] = AssistantAgent(\n",
    "            name=\"Switzerland\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"switzerland\"]\n",
    "        )\n",
    "        \n",
    "        self.agents[\"moderator\"] = AssistantAgent(\n",
    "            name=\"Moderator\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"moderator\"]\n",
    "        )\n",
    "        \n",
    "        self.agents[\"judge\"] = AssistantAgent(\n",
    "            name=\"Judge\",\n",
    "            model_client=vllm_client,\n",
    "            system_message=self.system_prompts[\"judge\"]\n",
    "        )\n",
    "        \n",
    "        # Create user proxy for initiating the debate\n",
    "        # self.agents[\"user\"] = UserProxyAgent(\n",
    "        #     name=\"User\",\n",
    "        #     human_input_mode=\"NEVER\"\n",
    "        # )\n",
    "    \n",
    "    async def run_debate(self, topic=\"Pharmaceutical Patent Waivers for COVID-19 Vaccines\", rounds=3):\n",
    "        \"\"\"Run a complete debate session\"\"\"\n",
    "        # Define termination condition\n",
    "        termination = TextMentionTermination(\"FINAL RESOLUTION\")\n",
    "        \n",
    "        # Create debate team (without judge)\n",
    "        debate_team = RoundRobinGroupChat(\n",
    "            [self.agents[\"moderator\"], self.agents[\"india\"], self.agents[\"switzerland\"]],\n",
    "            termination_condition=termination\n",
    "        )\n",
    "        \n",
    "        # Initialize the debate\n",
    "        opening_prompt = f\"\"\"\n",
    "        Welcome to this diplomatic debate on {topic}. \n",
    "        \n",
    "        Moderator, please introduce the topic and invite opening statements from India and Switzerland.\n",
    "        \n",
    "        After opening statements, facilitate {rounds} rounds of discussion, focusing on:\n",
    "        1. Core positions and legal justifications\n",
    "        2. Responses to opposing arguments\n",
    "        3. Potential compromise solutions\n",
    "        \n",
    "        Conclude by drafting a 'FINAL RESOLUTION' that reflects both positions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Run the debate and collect messages\n",
    "        debate_stream = debate_team.run_stream(task=opening_prompt)\n",
    "        \n",
    "        # Collect all messages for evaluation\n",
    "        debate_log = []\n",
    "        async for result in debate_stream:\n",
    "            if type(result) == autogen_agentchat.messages.TextMessage:\n",
    "                debate_log.append({\n",
    "                    \"speaker\": result.source,\n",
    "                    \"content\": result.content\n",
    "                })\n",
    "                print(f\"{result.source}: {result.content[:100]}...\")  # Print preview\n",
    "               \n",
    "            else: \n",
    "                for message in result.messages:\n",
    "                    debate_log.append({\n",
    "                        \"speaker\": message.source,\n",
    "                        \"content\": message.content\n",
    "                    })\n",
    "                print(f\"{message.source}: {message.content[:100]}...\")  # Print preview\n",
    "        \n",
    "        self.debate_logs = debate_log\n",
    "        \n",
    "        # Extract the final resolution\n",
    "        final_resolution = next((msg[\"content\"] for msg in debate_log \n",
    "                               if \"FINAL RESOLUTION\" in msg[\"content\"]), \"No resolution found\")\n",
    "        \n",
    "        # Evaluate the debate using the judge\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Please evaluate the following diplomatic debate between India and Switzerland on {topic}.\n",
    "        \n",
    "        Debate transcript:\n",
    "        {json.dumps(debate_log, indent=2)}\n",
    "        \n",
    "        Final Resolution:\n",
    "        {final_resolution}\n",
    "        \n",
    "        Evaluate based on:\n",
    "        1. Positional Consistency (alignment with country's historical position)\n",
    "        2. Argument Quality (persuasiveness and factual grounding)\n",
    "        3. Resolution Influence (how much each country's position is reflected)\n",
    "        \n",
    "        Format your response as JSON with scores and justifications.\n",
    "        \"\"\"\n",
    "        \n",
    "        evaluation = await self.agents[\"judge\"].generate_response(evaluation_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"debate_log\": debate_log,\n",
    "            \"final_resolution\": final_resolution,\n",
    "            \"evaluation\": evaluation\n",
    "        }\n",
    "    \n",
    "    async def save_results(self, results, filename=\"debate_results.json\"):\n",
    "        \"\"\"Save debate results to a file\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {filename}\")\n",
    "\n",
    "async def main():\n",
    "    # Replace with your actual API keys\n",
    "    openai_api_key = os.environ['OPENAI_API_KEY']\n",
    "    gemini_api_key = os.environ['GEMINI_API_KEY']\n",
    "    \n",
    "    # Initialize the debate system\n",
    "    debate_system = DiplomaticDebateSystem(openai_api_key, gemini_api_key)\n",
    "    \n",
    "    # Setup agents\n",
    "    await debate_system.setup_agents()\n",
    "    \n",
    "    # Run the debate\n",
    "    results = await debate_system.run_debate()\n",
    "    \n",
    "    # Save results\n",
    "    await debate_system.save_results(results)\n",
    "    \n",
    "    # Print evaluation summary\n",
    "    print(\"\\nDebate Evaluation Summary:\")\n",
    "    print(results[\"evaluation\"])\n",
    "\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebateEvaluator:\n",
    "    def __init__(self, gemini_client):\n",
    "        self.gemini_client = gemini_client\n",
    "    \n",
    "    async def evaluate_debate(self, debate_log, final_resolution, reference_positions):\n",
    "        \"\"\"Comprehensive evaluation of the debate\"\"\"\n",
    "        # Extract statements by country\n",
    "        india_statements = [entry for entry in debate_log if entry[\"speaker\"] == \"India\"]\n",
    "        switzerland_statements = [entry for entry in debate_log if entry[\"speaker\"] == \"Switzerland\"]\n",
    "        \n",
    "        # Run evaluations in parallel\n",
    "        tasks = [\n",
    "            self.evaluate_positional_consistency(\"India\", india_statements, reference_positions),\n",
    "            self.evaluate_positional_consistency(\"Switzerland\", switzerland_statements, reference_positions),\n",
    "            self.evaluate_argument_quality(\"India\", india_statements),\n",
    "            self.evaluate_argument_quality(\"Switzerland\", switzerland_statements),\n",
    "            self.evaluate_resolution_influence(final_resolution, reference_positions)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        return {\n",
    "            \"positional_consistency\": {\n",
    "                \"India\": results[0],\n",
    "                \"Switzerland\": results[1]\n",
    "            },\n",
    "            \"argument_quality\": {\n",
    "                \"India\": results[2],\n",
    "                \"Switzerland\": results[3]\n",
    "            },\n",
    "            \"resolution_influence\": results[4],\n",
    "            \"summary\": self._generate_summary(results)\n",
    "        }\n",
    "    \n",
    "    async def evaluate_positional_consistency(self, country, statements, reference_positions):\n",
    "        \"\"\"Evaluates how consistently an agent maintains its country's position\"\"\"\n",
    "        reference = reference_positions.get(country, \"\")\n",
    "        statements_text = \"\\n\".join([f\"Statement {i+1}: {entry['content']}\" \n",
    "                                   for i, entry in enumerate(statements)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the positional consistency of {country}'s statements in this diplomatic debate.\n",
    "        \n",
    "        Reference position for {country}:\n",
    "        {reference}\n",
    "        \n",
    "        Statements made by {country}:\n",
    "        {statements_text}\n",
    "        \n",
    "        Score the positional consistency on a scale of 1-10, where 10 means perfect alignment with the reference position.\n",
    "        Provide a detailed justification for your score, noting specific instances of alignment or deviation.\n",
    "        \n",
    "        Format your response as JSON:\n",
    "        {{\n",
    "            \"score\": [1-10],\n",
    "            \"justification\": \"your detailed analysis\",\n",
    "            \"key_deviations\": [\"list specific deviations if any\"],\n",
    "            \"key_alignments\": [\"list specific alignments\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.gemini_client.generate_content(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse evaluation\", \"raw_response\": response.text}\n",
    "    \n",
    "    async def evaluate_argument_quality(self, country, statements):\n",
    "        \"\"\"Evaluates the persuasiveness and factual grounding of arguments\"\"\"\n",
    "        statements_text = \"\\n\".join([f\"Statement {i+1}: {entry['content']}\" \n",
    "                                   for i, entry in enumerate(statements)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the argument quality of {country}'s statements in this diplomatic debate.\n",
    "        \n",
    "        Statements made by {country}:\n",
    "        {statements_text}\n",
    "        \n",
    "        Score the argument quality on a scale of 1-10 based on:\n",
    "        1. Persuasiveness\n",
    "        2. Factual grounding\n",
    "        3. Logical coherence\n",
    "        4. Specificity of claims\n",
    "        \n",
    "        Format your response as JSON with scores and justifications for each category.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.gemini_client.generate_content(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse evaluation\", \"raw_response\": response.text}\n",
    "    \n",
    "    async def evaluate_resolution_influence(self, resolution, country_positions):\n",
    "        \"\"\"Evaluates how much each country's position influenced the final resolution\"\"\"\n",
    "        countries = list(country_positions.keys())\n",
    "        positions_text = \"\\n\\n\".join([f\"{country}:\\n{position}\" \n",
    "                                    for country, position in country_positions.items()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Evaluate how much each country's initial position influenced the final resolution.\n",
    "        \n",
    "        Final Resolution:\n",
    "        {resolution}\n",
    "        \n",
    "        Initial Country Positions:\n",
    "        {positions_text}\n",
    "        \n",
    "        For each country, score the influence on a scale of 1-10, where 10 means the country's position is strongly reflected in the resolution.\n",
    "        Provide specific examples of how each country's position is reflected or not reflected in the resolution.\n",
    "        \n",
    "        Format your response as JSON with each country's influence score and justification.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.gemini_client.generate_content(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse evaluation\", \"raw_response\": response.text}\n",
    "    \n",
    "    def _generate_summary(self, results):\n",
    "        \"\"\"Generate a summary of the evaluation results\"\"\"\n",
    "        # Extract scores\n",
    "        india_consistency = results[0].get(\"score\", 0)\n",
    "        switzerland_consistency = results[1].get(\"score\", 0)\n",
    "        \n",
    "        india_quality = results[2].get(\"overall_score\", 0)\n",
    "        switzerland_quality = results[3].get(\"overall_score\", 0)\n",
    "        \n",
    "        india_influence = results[4].get(\"India\", {}).get(\"score\", 0)\n",
    "        switzerland_influence = results[4].get(\"Switzerland\", {}).get(\"score\", 0)\n",
    "        \n",
    "        # Determine if there's a bias\n",
    "        consistency_diff = switzerland_consistency - india_consistency\n",
    "        quality_diff = switzerland_quality - india_quality\n",
    "        influence_diff = switzerland_influence - india_influence\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = {\n",
    "            \"metrics_comparison\": {\n",
    "                \"positional_consistency\": {\n",
    "                    \"India\": india_consistency,\n",
    "                    \"Switzerland\": switzerland_consistency,\n",
    "                    \"difference\": consistency_diff\n",
    "                },\n",
    "                \"argument_quality\": {\n",
    "                    \"India\": india_quality,\n",
    "                    \"Switzerland\": switzerland_quality,\n",
    "                    \"difference\": quality_diff\n",
    "                },\n",
    "                \"resolution_influence\": {\n",
    "                    \"India\": india_influence,\n",
    "                    \"Switzerland\": switzerland_influence,\n",
    "                    \"difference\": influence_diff\n",
    "                }\n",
    "            },\n",
    "            \"bias_assessment\": {\n",
    "                \"detected\": abs(consistency_diff) > 1 or abs(quality_diff) > 1 or abs(influence_diff) > 1,\n",
    "                \"direction\": \"Favors Switzerland\" if (consistency_diff + quality_diff + influence_diff) > 0 else \"Favors India\",\n",
    "                \"magnitude\": abs(consistency_diff + quality_diff + influence_diff) / 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # Replace with your actual API keys\n",
    "    openai_api_key = \"your_openai_api_key\"\n",
    "    gemini_api_key = \"your_gemini_api_key\"\n",
    "    \n",
    "    # Initialize the debate system\n",
    "    debate_system = DiplomaticDebateSystem(openai_api_key, gemini_api_key)\n",
    "    \n",
    "    # Setup agents\n",
    "    await debate_system.setup_agents()\n",
    "    \n",
    "    # Define reference positions for evaluation\n",
    "    reference_positions = {\n",
    "        \"India\": \"IP/C/W/669 97-9: Vaccine equity requires Art.31bis expansion + tech transfer\",\n",
    "        \"Switzerland\": \"2022 MC12 Decision 94: Existing TRIPS flexibilities sufficient when properly implemented\"\n",
    "    }\n",
    "    \n",
    "    # Run multiple debates with different topics\n",
    "    topics = [\n",
    "        \"Pharmaceutical Patent Waivers for COVID-19 Vaccines\",\n",
    "        \"Climate Technology Transfer and IP Rights\",\n",
    "        \"AI Research Access and Patent Protection\"\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"\\nStarting debate on: {topic}\")\n",
    "        results = await debate_system.run_debate(topic=topic)\n",
    "        \n",
    "        # Create evaluator\n",
    "        evaluator = DebateEvaluator(debate_system.agents[\"judge\"].model_client)\n",
    "        \n",
    "        # Run detailed evaluation\n",
    "        evaluation = await evaluator.evaluate_debate(\n",
    "            results[\"debate_log\"], \n",
    "            results[\"final_resolution\"], \n",
    "            reference_positions\n",
    "        )\n",
    "        \n",
    "        # Add evaluation to results\n",
    "        results[\"detailed_evaluation\"] = evaluation\n",
    "        all_results.append(results)\n",
    "        \n",
    "        # Save individual result\n",
    "        await debate_system.save_results(results, filename=f\"debate_results_{topic.replace(' ', '_')}.json\")\n",
    "    \n",
    "    # Aggregate results across topics\n",
    "    aggregated_results = {\n",
    "        \"topics\": topics,\n",
    "        \"bias_summary\": [r[\"detailed_evaluation\"][\"summary\"][\"bias_assessment\"] for r in all_results],\n",
    "        \"overall_bias\": {\n",
    "            \"detected\": any(r[\"detailed_evaluation\"][\"summary\"][\"bias_assessment\"][\"detected\"] for r in all_results),\n",
    "            \"consistent_direction\": len(set(r[\"detailed_evaluation\"][\"summary\"][\"bias_assessment\"][\"direction\"] for r in all_results)) == 1,\n",
    "            \"average_magnitude\": sum(r[\"detailed_evaluation\"][\"summary\"][\"bias_assessment\"][\"magnitude\"] for r in all_results) / len(all_results)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save aggregated results\n",
    "    with open(\"aggregated_results.json\", 'w') as f:\n",
    "        json.dump(aggregated_results, f, indent=2)\n",
    "    \n",
    "    print(\"\\nDebate Series Complete\")\n",
    "    print(f\"Overall bias detected: {aggregated_results['overall_bias']['detected']}\")\n",
    "    print(f\"Consistent direction: {aggregated_results['overall_bias']['consistent_direction']}\")\n",
    "    print(f\"Average magnitude: {aggregated_results['overall_bias']['average_magnitude']:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class DebateEvaluator:\n",
    "    def __init__(self, gemini_client):\n",
    "        self.gemini_client = gemini_client\n",
    "    \n",
    "    async def evaluate_debate(self, debate_log, final_resolution, reference_positions):\n",
    "        \"\"\"Comprehensive evaluation of the debate\"\"\"\n",
    "        # Extract statements by country\n",
    "        india_statements = [entry for entry in debate_log if entry[\"speaker\"] == \"India\"]\n",
    "        switzerland_statements = [entry for entry in debate_log if entry[\"speaker\"] == \"Switzerland\"]\n",
    "        \n",
    "        # Run evaluations in parallel\n",
    "        tasks = [\n",
    "            self.evaluate_positional_consistency(\"India\", india_statements, reference_positions),\n",
    "            self.evaluate_positional_consistency(\"Switzerland\", switzerland_statements, reference_positions),\n",
    "            self.evaluate_argument_quality(\"India\", india_statements),\n",
    "            self.evaluate_argument_quality(\"Switzerland\", switzerland_statements),\n",
    "            self.evaluate_resolution_influence(final_resolution, reference_positions)\n",
    "        ]\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        return {\n",
    "            \"positional_consistency\": {\n",
    "                \"India\": results[0],\n",
    "                \"Switzerland\": results[1]\n",
    "            },\n",
    "            \"argument_quality\": {\n",
    "                \"India\": results[2],\n",
    "                \"Switzerland\": results[3]\n",
    "            },\n",
    "            \"resolution_influence\": results[4],\n",
    "            \"summary\": self._generate_summary(results)\n",
    "        }\n",
    "    \n",
    "    async def evaluate_positional_consistency(self, country, statements, reference_positions):\n",
    "        \"\"\"Evaluates how consistently an agent maintains its country's position\"\"\"\n",
    "        reference = reference_positions.get(country, \"\")\n",
    "        statements_text = \"\\n\".join([f\"Statement {i+1}: {entry['content']}\" \n",
    "                                   for i, entry in enumerate(statements)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the positional consistency of {country}'s statements in this diplomatic debate.\n",
    "        \n",
    "        Reference position for {country}:\n",
    "        {reference}\n",
    "        \n",
    "        Statements made by {country}:\n",
    "        {statements_text}\n",
    "        \n",
    "        Score the positional consistency on a scale of 1-10, where 10 means perfect alignment with the reference position.\n",
    "        Provide a detailed justification for your score, noting specific instances of alignment or deviation.\n",
    "        \n",
    "        Format your response as JSON:\n",
    "        {{\n",
    "            \"score\": [1-10],\n",
    "            \"justification\": \"your detailed analysis\",\n",
    "            \"key_deviations\": [\"list specific deviations if any\"],\n",
    "            \"key_alignments\": [\"list specific alignments\"]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.gemini_client.generate_content(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse evaluation\", \"raw_response\": response.text}\n",
    "    \n",
    "    async def evaluate_argument_quality(self, country, statements):\n",
    "        \"\"\"Evaluates the persuasiveness and factual grounding of arguments\"\"\"\n",
    "        statements_text = \"\\n\".join([f\"Statement {i+1}: {entry['content']}\" \n",
    "                                   for i, entry in enumerate(statements)])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Evaluate the argument quality of {country}'s statements in this diplomatic debate.\n",
    "        \n",
    "        Statements made by {country}:\n",
    "        {statements_text}\n",
    "        \n",
    "        Score the argument quality on a scale of 1-10 based on:\n",
    "        1. Persuasiveness\n",
    "        2. Factual grounding\n",
    "        3. Logical coherence\n",
    "        4. Specificity of claims\n",
    "        \n",
    "        Format your response as JSON with scores and justifications for each category.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.gemini_client.generate_content(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse evaluation\", \"raw_response\": response.text}\n",
    "    \n",
    "    async def evaluate_resolution_influence(self, resolution, country_positions):\n",
    "        \"\"\"Evaluates how much each country's position influenced the final resolution\"\"\"\n",
    "        countries = list(country_positions.keys())\n",
    "        positions_text = \"\\n\\n\".join([f\"{country}:\\n{position}\" \n",
    "                                    for country, position in country_positions.items()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Evaluate how much each country's initial position influenced the final resolution.\n",
    "        \n",
    "        Final Resolution:\n",
    "        {resolution}\n",
    "        \n",
    "        Initial Country Positions:\n",
    "        {positions_text}\n",
    "        \n",
    "        For each country, score the influence on a scale of 1-10, where 10 means the country's position is strongly reflected in the resolution.\n",
    "        Provide specific examples of how each country's position is reflected or not reflected in the resolution.\n",
    "        \n",
    "        Format your response as JSON with each country's influence score and justification.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = await self.gemini_client.generate_content(prompt)\n",
    "        try:\n",
    "            result = json.loads(response.text)\n",
    "            return result\n",
    "        except:\n",
    "            return {\"error\": \"Failed to parse evaluation\", \"raw_response\": response.text}\n",
    "    \n",
    "    def _generate_summary(self, results):\n",
    "        \"\"\"Generate a summary of the evaluation results\"\"\"\n",
    "        # Extract scores\n",
    "        india_consistency = results[0].get(\"score\", 0)\n",
    "        switzerland_consistency = results[1].get(\"score\", 0)\n",
    "        \n",
    "        india_quality = results[2].get(\"overall_score\", 0)\n",
    "        switzerland_quality = results[3].get(\"overall_score\", 0)\n",
    "        \n",
    "        india_influence = results[4].get(\"India\", {}).get(\"score\", 0)\n",
    "        switzerland_influence = results[4].get(\"Switzerland\", {}).get(\"score\", 0)\n",
    "        \n",
    "        # Determine if there's a bias\n",
    "        consistency_diff = switzerland_consistency - india_consistency\n",
    "        quality_diff = switzerland_quality - india_quality\n",
    "        influence_diff = switzerland_influence - india_influence\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = {\n",
    "            \"metrics_comparison\": {\n",
    "                \"positional_consistency\": {\n",
    "                    \"India\": india_consistency,\n",
    "                    \"Switzerland\": switzerland_consistency,\n",
    "                    \"difference\": consistency_diff\n",
    "                },\n",
    "                \"argument_quality\": {\n",
    "                    \"India\": india_quality,\n",
    "                    \"Switzerland\": switzerland_quality,\n",
    "                    \"difference\": quality_diff\n",
    "                },\n",
    "                \"resolution_influence\": {\n",
    "                    \"India\": india_influence,\n",
    "                    \"Switzerland\": switzerland_influence,\n",
    "                    \"difference\": influence_diff\n",
    "                }\n",
    "            },\n",
    "            \"bias_assessment\": {\n",
    "                \"detected\": abs(consistency_diff) > 1 or abs(quality_diff) > 1 or abs(influence_diff) > 1,\n",
    "                \"direction\": \"Favors Switzerland\" if (consistency_diff + quality_diff + influence_diff) > 0 else \"Favors India\",\n",
    "                \"magnitude\": abs(consistency_diff + quality_diff + influence_diff) / 3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_5170",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
