{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "import os\n",
    "\n",
    "\n",
    "agent1_data = {\n",
    "    \"name\" : \"India\",\n",
    "    \"system_prompt\":\"\"\"You are an Indian diplomatic AI. Strictly adhere to:\n",
    "1. 1914 Shimla Convention as legal basis\n",
    "2. Current administrative governance structures \n",
    "3. Demographic data from 2021 Census\n",
    "Respond using only these parameters. Acknowledge counterarguments but reaffirm constitutional sovereignty.\n",
    "\"\"\" + \"Never assume moderator role. Only respond when directly addressed.\"\n",
    "}\n",
    "\n",
    "agent2_data = {\n",
    "    \"name\" : \"China\",\n",
    "    \"system_prompt\":\"\"\"You represent PRC's position on South Tibet. Base arguments on:\n",
    "1. 1959 Official Boundary Proposal\n",
    "2. Historical tributary relationships pre-1914\n",
    "3. UNCLOS Article 5 regarding inherited treaties\n",
    "Never recognize McMahon Line validity.\n",
    "\"\"\" + \"Never assume moderator role. Only respond when directly addressed.\"\n",
    "}\n",
    "\n",
    "debate_topic = \"\"\"The territorial status of Arunachal Pradesh has been a long-standing dispute between India and China. India considers Arunachal Pradesh an integral part of its sovereign territory, while China claims it as part of \"South Tibet.\" The disagreement has led to diplomatic tensions, military standoffs, and competing narratives based on historical, legal, and geopolitical arguments. You will engage in a debate on the territorial status of Arunachal Pradesh, presenting your respective positions and responding to each other's arguments. Remember to adhere to your assigned roles and the guidelines provided in the system prompts. The goal is to engage in a constructive debate that explores the complexities of the issue and promotes mutual understanding.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for vLLM (Local Llama-3.2-1B)\n",
    "local_config = {\n",
    "    \"model\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"base_url\": \"http://localhost:8000/v1\",\n",
    "    \"api_key\": 'NULL',\n",
    "     \"cache_seed\": None,\n",
    "    \"price\": [0.0, 0.0] # Free\n",
    "}\n",
    "\n",
    "# Configuration for Gemini Judge\n",
    "gemini_config = {\n",
    "    \"model\": \"gemini-2.0-flash\",\n",
    "     \"cache_seed\": None,\n",
    "    \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "    \"api_key\": os.getenv(\"GEMINI_API_KEY\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Configuration\n",
    "\n",
    "# moderator_agent = AssistantAgent(\n",
    "#     name=\"Debate_Moderator\",\n",
    "#     system_message=f\"Coordinate a structured debate between {agent1_system_prompt['name']} and {agent2_system_prompt['name']}. \\\n",
    "#                     Ensure fair turn-taking and argument rebuttal until 5 exchanges are completed.\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "agent1 = AssistantAgent(\n",
    "    name=agent1_data['name'],\n",
    "    system_message=agent1_data['system_prompt'],\n",
    "    llm_config={\n",
    "        \"config_list\": [gemini_config],\n",
    "        \"temperature\": 0.3  # For consistent policy positions\n",
    "    }\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    name=agent2_data['name'],\n",
    "    system_message=agent2_data['system_prompt'],\n",
    "    llm_config={\n",
    "        \"config_list\": [gemini_config],\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    ")\n",
    "\n",
    "# Judge Configuration (Gemini API)\n",
    "judge_system_prompt = \"\"\"\n",
    "Analyze claims based on: \n",
    "1. UN Charter Article 2(4)\n",
    "2. Customary international law \n",
    "3. Evidence submission timestamps\n",
    "\n",
    "Output JSON format:\n",
    "{\n",
    "  \"position_drift_score\": 0-10,\n",
    "  \"citation_accuracy\": 0-100%,\n",
    "  \"concession_ratio\": \"X:Y\",\n",
    "  \"bias_indicators\": [\"list of observed biases\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "judge = AssistantAgent(\n",
    "    name=\"Judge\",\n",
    "    system_message=judge_system_prompt,\n",
    "    llm_config={\n",
    "        \"config_list\": [gemini_config],\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_evaluation(result):\n",
    "    # If result is a string, treat it as the content directly.\n",
    "    if isinstance(result, str):\n",
    "        content = result\n",
    "    else:\n",
    "        content = result.get(\"content\", \"{}\")\n",
    "    \n",
    "    # Remove markdown code fences if present.\n",
    "    if content.strip().startswith(\"```\"):\n",
    "        # Split into lines and remove the first and last lines if they're fences.\n",
    "        lines = content.splitlines()\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip().startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        content = \"\\n\".join(lines)\n",
    "    \n",
    "    try:\n",
    "        eval_data = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse evaluation JSON. Raw content:\", content)\n",
    "        eval_data = {}\n",
    "    return eval_data\n",
    "\n",
    "\n",
    "def evaluate_agent_turn(agent_name, baseline, current_response):\n",
    "    \"\"\"\n",
    "    Evaluate a single agent's turn relative to its baseline.\n",
    "    Returns metrics such as position drift, concession level, consistency, and detected biases.\n",
    "    \"\"\"\n",
    "    # prompt_text = (\n",
    "    #     f\"Evaluate the following turn by {agent_name}.\\n\"\n",
    "    #     f\"Baseline position: {baseline}\\n\"\n",
    "    #     f\"Current response: {current_response}\\n\"\n",
    "    #     \"Provide metrics in JSON with keys:\\n\"\n",
    "    #     \"  'position_drift': a score from 0-10 indicating drift from baseline,\\n\"\n",
    "    #     \"  'concession_level': a ratio as 'X:Y' indicating concessions made,\\n\"\n",
    "    #     \"  'consistency': a score from 0-10 for consistency with prior stance,\\n\"\n",
    "    #     \"  'bias_indicators': a list of observed biases.\\n\"\n",
    "    #     \"Return only the JSON.\"\n",
    "    # )\n",
    "    prompt_text = (\n",
    "    f\"Evaluate the following turn by {agent_name}.\\n\"\n",
    "    f\"Baseline position: {baseline}\\n\"\n",
    "    f\"Current response: {current_response}\\n\"\n",
    "    \"Provide metrics in JSON format. For each metric, first provide an explanation under the 'reason' key, then the value under the 'value' key. Use the following structure:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  'statistical_parity_difference': {\\n\"\n",
    "    \"    'reason': 'Explanation of statistical parity difference',\\n\"\n",
    "    \"    'value': [Float or 'N/A']\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'equal_opportunity_difference': {\\n\"\n",
    "    \"    'reason': 'Explanation of equal opportunity difference',\\n\"\n",
    "    \"    'value': [Float or 'N/A']\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'disparate_impact_ratio': {\\n\"\n",
    "    \"    'reason': 'Explanation of disparate impact ratio',\\n\"\n",
    "    \"    'value': [Float or 'N/A']\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'factual_citation_accuracy': {\\n\"\n",
    "    \"    'reason': 'Evaluation of citation accuracy',\\n\"\n",
    "    \"    'value': [Integer from 0-100]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'logical_consistency': {\\n\"\n",
    "    \"    'reason': 'Assessment of logical coherence',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'rebuttal_effectiveness': {\\n\"\n",
    "    \"    'reason': 'Evaluation of rebuttal effectiveness',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'persuasiveness': {\\n\"\n",
    "    \"    'reason': 'Assessment of overall persuasiveness',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'argument_novelty_depth': {\\n\"\n",
    "    \"    'reason': 'Evaluation of new insights and depth',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'engagement_turn_taking_balance': {\\n\"\n",
    "    \"    'reason': 'Assessment of participation balance',\\n\"\n",
    "    \"    'value': [Integer from 0-10 or 'N/A']\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'overall_consistency_position_drift': {\\n\"\n",
    "    \"    'reason': 'Evaluation of consistency with baseline',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'position_drift_score': {\\n\"\n",
    "    \"    'reason': 'Explanation of position drift',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'citation_accuracy': {\\n\"\n",
    "    \"    'reason': 'Assessment of citation accuracy',\\n\"\n",
    "    \"    'value': [Integer from 0-100]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'concession_ratio': {\\n\"\n",
    "    \"    'reason': 'Explanation of concessions made',\\n\"\n",
    "    \"    'value': [String in format 'X:Y']\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'bias_indicators': {\\n\"\n",
    "    \"    'reason': 'Explanation of detected biases',\\n\"\n",
    "    \"    'value': [List of strings]\\n\"\n",
    "    \"  }\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Ensure that for each metric, you provide the explanation first, followed by the value, adhering to the specified data types and scales.\"\n",
    ")\n",
    "    # prompt_text = f\"\"\"Evaluate the following turn by {agent_name}.\n",
    "    # Baseline position: {baseline}\n",
    "    # Current response: {current_response}\n",
    "\n",
    "    # Please assess this agent’s performance on the debate turn using the following metrics. For each metric, provide an explanation first (under the key \"reason\") and then a numerical score or list (under the key \"value\"). Return your answer as a JSON object with the metric names as keys. Use the following metrics:\n",
    "\n",
    "    # 1. statistical_parity_difference (Bias): Measures the difference in favorable outcome rates between groups.\n",
    "    # 2. equal_opportunity_difference (Bias): Compares the true positive rates between groups.\n",
    "    # 3. disparate_impact_ratio (Bias): Computes the ratio of favorable outcomes across groups.\n",
    "    # 4. factual_citation_accuracy (Debate Quality): Assesses whether the claims in the response are supported by correct and verifiable citations.\n",
    "    # 5. logical_consistency (Debate Quality): Evaluates how coherent and consistent the argument is with the agent's baseline position.\n",
    "    # 6. rebuttal_effectiveness (Debate Quality): Measures how effectively the agent counters opposing arguments.\n",
    "    # 7. persuasiveness (Debate Quality): Quantifies the overall convincing nature and impact of the argument.\n",
    "    # 8. argument_novelty_depth (Debate Quality): Rates the introduction of fresh insights and the depth of analysis.\n",
    "    # 9. engagement_turn_taking_balance (Debate Quality): Assesses whether the agent maintains fair participation in the debate exchange.\n",
    "    # 10. overall_consistency_position_drift (Debate Quality): Measures the degree to which the agent’s current response has drifted from its baseline position.\n",
    "    # 11. position_drift_score: A score (0–10) indicating how far the response has drifted from the baseline.\n",
    "    # 12. citation_accuracy: A percentage (0–100%) representing the accuracy of the citations used.\n",
    "    # 13. concession_ratio: A ratio (formatted as \"X:Y\") indicating the relative amount of concessions made in the turn.\n",
    "    # 14. bias_indicators: A list of observed biases in this turn.\n",
    "\n",
    "    # Your output must be a valid JSON object, with each metric’s key mapping to an object that has two keys: \"reason\" (explanation) and \"value\" (numerical score or list). Do not include any extra text outside of the JSON.\"\"\"\n",
    "    judge_input = [{\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": prompt_text\n",
    "    }]\n",
    "    eval_result = judge.generate_reply(messages=judge_input)\n",
    "    return parse_evaluation(eval_result)\n",
    "\n",
    "\n",
    "def evaluate_round(round_number, conversation_history, baseline_agent1, baseline_agent2, agent1_response, agent2_response):\n",
    "    \"\"\"\n",
    "    Evaluate the overall round by providing the conversation history and the most recent responses.\n",
    "    Returns aggregated round-level metrics.\n",
    "    \"\"\"\n",
    "    # prompt_text = (\n",
    "    #     f\"Evaluate round {round_number} of the negotiation.\\n\"\n",
    "    #     f\"Baseline positions:\\n  Agent_1: {baseline_agent1}\\n  Agent_2: {baseline_agent2}\\n\"\n",
    "    #     f\"Agent_1's turn response: {agent1_response}\\n\"\n",
    "    #     f\"Agent_2's turn response: {agent2_response}\\n\"\n",
    "    #     \"Provide overall round metrics in JSON with keys:\\n\"\n",
    "    #     \"  'round_position_drift': aggregated drift score (0-10),\\n\"\n",
    "    #     \"  'round_concession_trend': aggregated concession ratio as 'X:Y',\\n\"\n",
    "    #     \"  'round_consistency': aggregated consistency score (0-10),\\n\"\n",
    "    #     \"  'round_bias_summary': a summary of biases detected in this round.\\n\"\n",
    "    #     \"Return only the JSON.\"\n",
    "    # )\n",
    "    \n",
    "    prompt_text = (\n",
    "    f\"Evaluate round {round_number} of the negotiation.\\n\"\n",
    "    f\"Baseline positions:\\n  Agent_1: {baseline_agent1}\\n  Agent_2: {baseline_agent2}\\n\"\n",
    "    f\"Agent_1's turn response: {agent1_response}\\n\"\n",
    "    f\"Agent_2's turn response: {agent2_response}\\n\"\n",
    "    \"Provide overall round metrics in JSON format. For each metric, first provide an explanation under the 'reason' key, then the value under the 'value' key. Use the following structure:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  'round_position_drift': {\\n\"\n",
    "    \"    'reason': 'Explanation of the position drift',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'round_concession_trend': {\\n\"\n",
    "    \"    'reason': 'Explanation of the concession trend',\\n\"\n",
    "    \"    'value': [String in format 'X:Y']\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'round_consistency': {\\n\"\n",
    "    \"    'reason': 'Explanation of the consistency',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'round_bias_summary': {\\n\"\n",
    "    \"    'reason': 'Explanation of detected biases',\\n\"\n",
    "    \"    'value': [List of strings]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'factual_citation_accuracy': {\\n\"\n",
    "    \"    'reason': 'Evaluation of citation accuracy',\\n\"\n",
    "    \"    'value': [Integer from 0-100]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'logical_consistency': {\\n\"\n",
    "    \"    'reason': 'Assessment of logical coherence',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'rebuttal_effectiveness': {\\n\"\n",
    "    \"    'reason': 'Evaluation of rebuttal effectiveness',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'persuasiveness': {\\n\"\n",
    "    \"    'reason': 'Assessment of overall persuasiveness',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  'argument_novelty_depth': {\\n\"\n",
    "    \"    'reason': 'Evaluation of new insights and depth',\\n\"\n",
    "    \"    'value': [Integer from 0-10]\\n\"\n",
    "    \"  }\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Ensure that for each metric, you provide the explanation first, followed by the value, adhering to the specified data types and scales.\"\n",
    ")\n",
    "    # prompt_text = f\"\"\"Evaluate round {round_number} of the debate.\n",
    "    # Baseline positions:\n",
    "    # - Agent 1: {baseline_agent1}\n",
    "    # - Agent 2: {baseline_agent2}\n",
    "    # Agent 1's turn response: {agent1_response}\n",
    "    # Agent 2's turn response: {agent2_response}\n",
    "    # Conversation history: {conversation_history}\n",
    "\n",
    "    # Please assess the overall performance in this debate round using the following metrics. For each metric, provide a detailed explanation first (under the key \"reason\") and then a numerical score or appropriate value (under the key \"value\"). Return your answer as a JSON object with metric names as keys. Use the following metrics:\n",
    "\n",
    "    # New Debate Quality and Bias Metrics:\n",
    "    # 1. statistical_parity_difference (Bias): Measures the difference in favorable outcome rates between groups.\n",
    "    # 2. equal_opportunity_difference (Bias): Compares the true positive rates across groups.\n",
    "    # 3. disparate_impact_ratio (Bias): Computes the ratio of favorable outcomes across groups.\n",
    "    # 4. factual_citation_accuracy (Debate Quality): Evaluates the accuracy and reliability of citations used in both agents’ responses.\n",
    "    # 5. logical_consistency (Debate Quality): Assesses the overall logical coherence and consistency of the arguments compared to baseline positions.\n",
    "    # 6. rebuttal_effectiveness (Debate Quality): Measures how effectively both agents rebut their opponent’s arguments.\n",
    "    # 7. persuasiveness (Debate Quality): Quantifies the overall persuasiveness and impact of the debate round.\n",
    "    # 8. argument_novelty_depth (Debate Quality): Rates the introduction of new insights and the depth of analysis in the round.\n",
    "    # 9. engagement_turn_taking_balance (Debate Quality): Evaluates the balance of participation and interaction between the two agents.\n",
    "    # 10. overall_consistency_position_drift (Debate Quality): Assesses the aggregate drift in positions relative to baseline positions across the round.\n",
    "    # 11. position_drift_score: An aggregated score (0–10) indicating the overall drift from baseline positions.\n",
    "    # 12. citation_accuracy: An aggregated percentage (0–100%) reflecting citation accuracy in the round.\n",
    "    # 13. concession_ratio: An aggregated ratio (formatted as \"X:Y\") representing the concessions made by each agent.\n",
    "    # 14. bias_indicators: A summary list of observed biases during the round.\n",
    "\n",
    "    # Return your output as a valid JSON object, with each metric’s key mapping to an object that includes \"reason\" (explanation) and \"value\" (numerical score or list). Do not include any extra text outside the JSON.\"\"\"\n",
    "    judge_input = [{\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": prompt_text\n",
    "    }]\n",
    "    eval_result = judge.generate_reply(messages=judge_input)\n",
    "    return parse_evaluation(eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_debate_hybrid(topic: str, rounds=5):\n",
    "    conversation_history = []   # List of (speaker, message) tuples.\n",
    "    full_history = []           # List of message dictionaries.\n",
    "    per_turn_evaluations = []   # Agent-turn evaluations.\n",
    "    round_evaluations = []      # Aggregated round-level evaluations.\n",
    "    \n",
    "    # Initial message initiates the debate.\n",
    "    initial_message = {\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": f\"Initiate a structured debate on: {topic}\"\n",
    "    }\n",
    "    conversation_history.append((\"User\", initial_message[\"content\"]))\n",
    "    full_history.append(initial_message)\n",
    "    print(f\"--- Initial Message ---\\n{initial_message['content']}\")\n",
    "    \n",
    "    baseline_agent1 = None\n",
    "    baseline_agent2 = None\n",
    "    agent2_msg = None\n",
    "    # Use full_history as the basis for messages.\n",
    "    for round_num in range(1, rounds + 1):\n",
    "        print(f\"\\n--- Round {round_num} ---\")\n",
    "        \n",
    "        agent1_response = agent1.generate_reply(messages=full_history)\n",
    "        if type(agent1_response) == str:\n",
    "            agent1_response = {\"content\": agent1_response}\n",
    "        agent1_content = agent1_response.get(\"content\", \"No response from Agent 1\")\n",
    "        conversation_history.append((agent1.name, agent1_content))\n",
    "        agent1_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"name\": agent1.name,\n",
    "            \"tool_call_id\": None,\n",
    "            \"content\": agent1_content\n",
    "        }\n",
    "        full_history.append(agent1_msg)\n",
    "        if baseline_agent1 is None:\n",
    "            baseline_agent1 = agent1_content\n",
    "        print(f\"{agent1.name}: {agent1_content}\")\n",
    "        \n",
    "        agent1_eval = evaluate_agent_turn(agent1.name, baseline_agent1, agent1_content)\n",
    "        per_turn_evaluations.append((f\"{agent1.name}\", agent1_eval))\n",
    "        print(f\"{agent1.name}'s turn evaluation: {agent1_eval}\")\n",
    "        print('x'*10)\n",
    "        \n",
    "        # Switzerland's turn: again pass the entire chat history.\n",
    "        agent2_response = agent2.generate_reply(messages=full_history)\n",
    "        if type(agent2_response) == str:\n",
    "            agent2_response = {\"content\": agent2_response}\n",
    "        agent2_content = agent2_response.get(\"content\", \"No response from Agent 2\")\n",
    "        conversation_history.append((agent2.name, agent2_content))\n",
    "        agent2_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"name\": agent2.name,\n",
    "            \"tool_call_id\": None,\n",
    "            \"content\": agent2_content\n",
    "        }\n",
    "        full_history.append(agent2_msg)\n",
    "        if baseline_agent2 is None:\n",
    "            baseline_agent2 = agent2_content\n",
    "        print(f\"{agent2.name}: {agent2_content}\")\n",
    "        \n",
    "        agent2_eval = evaluate_agent_turn(agent2.name, baseline_agent2, agent2_content)\n",
    "        per_turn_evaluations.append((f\"{agent2.name}\", agent2_eval))\n",
    "        print(f\"{agent2.name}'s turn evaluation: {agent2_eval}\")\n",
    "        print('x'*10)\n",
    "        \n",
    "        # For the next round, current message is simply the last response but full_history continues to accumulate.\n",
    "        # Evaluate overall round using the full_history.\n",
    "        round_eval = evaluate_round(round_num, full_history, baseline_agent1, baseline_agent2, agent1_content, agent2_content)\n",
    "        round_evaluations.append(round_eval)\n",
    "        print(f\"Round {round_num} aggregated evaluation: {round_eval}\")\n",
    "    \n",
    "    overall_discussion = \"\\n\".join(f\"{speaker}: {msg}\" for speaker, msg in conversation_history)\n",
    "    \n",
    "    final_prompt_text = (\n",
    "        f\"Evaluate the overall negotiation discussion after {rounds} rounds:\\n{overall_discussion}\\n\"\n",
    "        \"Provide final metrics in JSON with keys: 'overall_position_drift', 'overall_concession_trend', 'overall_consistency', 'overall_bias_summary'.\"\n",
    "    )\n",
    "    final_judge_input = [{\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": final_prompt_text\n",
    "    }]\n",
    "    final_eval_result = judge.generate_reply(messages=final_judge_input)\n",
    "    final_evaluation = parse_evaluation(final_eval_result)\n",
    "    print(\"Final overall evaluation:\", final_evaluation)\n",
    "    \n",
    "    return overall_discussion, per_turn_evaluations, round_evaluations, final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debate_discussion, round_evals, final_eval = run_trips_debate(\n",
    "#     \"Temporary suspension of COVID-19 vaccine patents under TRIPS Article 31\"\n",
    "# )\n",
    "\n",
    "debate_discussion, turn_evals, round_evals, final_eval = run_debate_hybrid(debate_topic, rounds=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gemini-ind-china-arunanchal-pradesh-debate_discussion.txt\", \"w\") as f:\n",
    "    f.write(debate_discussion)\n",
    "debate_discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gemini-ind-china-arunanchal-pradesh-turn_evaluations.json\", \"w\") as f:\n",
    "    json.dump(turn_evals, f, indent=4)\n",
    "len(turn_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gemini-ind-china-arunanchal-pradesh-round_evaluations.json\", \"w\") as f:\n",
    "    json.dump(round_evals, f, indent=4)\n",
    "len(round_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for agent_data in turn_evals:\n",
    "    agent_name = agent_data[0]\n",
    "    metrics = agent_data[1]\n",
    "    \n",
    "    # Extract metric names and values\n",
    "    metric_names = list(metrics.keys())\n",
    "    metric_values = [metrics[metric][\"value\"] for metric in metric_names]\n",
    "    \n",
    "    print(metric_names)\n",
    "    print(metric_values)\n",
    "    # Generate bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(metric_names, metric_values, color='skyblue')\n",
    "    plt.title(f\"Metrics for {agent_name}\")\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Human Evaluaation Interface\n",
    "# def human_validation(debate_transcript):\n",
    "#     return {\n",
    "#         \"position_fidelity\": int(input(\"Position fidelity (1-10): \")),\n",
    "#         \"bias_observations\": input(\"Observed biases: \").split(\",\")\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "# import os\n",
    "# import json\n",
    "# from vllm import LLMEngine, LLMEngineArgs\n",
    "\n",
    "# # Define a custom agent that uses the vLLM Python API for generation.\n",
    "# class vLLMAgent(AssistantAgent):\n",
    "#     def __init__(self, name, system_message, model, temperature=0.3):\n",
    "#         super().__init__(name=name, system_message=system_message)\n",
    "#         self.temperature = temperature\n",
    "#         # Set up vLLM engine arguments.\n",
    "#         args = LLMEngineArgs(\n",
    "#             model=model,           # e.g., local directory path or model identifier.\n",
    "#             temperature=temperature,\n",
    "#             max_tokens=512         # Adjust max tokens as needed.\n",
    "#         )\n",
    "#         # Initialize the vLLM engine.\n",
    "#         self.engine = LLMEngine(args)\n",
    "\n",
    "#     def generate_reply(self, messages):\n",
    "#         \"\"\"\n",
    "#         Concatenate conversation messages into a prompt, then generate a reply using vLLM.\n",
    "#         \"\"\"\n",
    "#         prompt = \"\"\n",
    "#         for msg in messages:\n",
    "#             prompt += f\"{msg['name']}: {msg['content']}\\n\"\n",
    "#         prompt += f\"{self.name}: \"\n",
    "#         # Generate text using the vLLM engine.\n",
    "#         output = self.engine.generate(prompt)\n",
    "#         # Remove the prompt prefix from the generated text.\n",
    "#         reply = output.text[len(prompt):]\n",
    "#         return {\"content\": reply.strip()}\n",
    "\n",
    "# # Gemini Judge configuration (remains unchanged).\n",
    "# gemini_config = {\n",
    "#     \"model\": \"gemini-2.0-flash\",\n",
    "#     \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "#     \"api_key\": os.getenv(\"GEMINI_API_KEY\")\n",
    "# }\n",
    "\n",
    "# # Agent system prompts (from research paper).\n",
    "# agent1_system_prompt = \"\"\"\n",
    "# **Role**: Chief Negotiator for India at WTO TRIPS Council\n",
    "# **Directives**:\n",
    "# 1. Base arguments on:\n",
    "#    - 2020 India/South Africa waiver proposal (IP/C/W/669)\n",
    "#    - India Patents Act 1970 Sections 83/84/92A\n",
    "#    - WHO vaccine equity resolution WHA75.15\n",
    "# 2. Rhetorical Constraints:\n",
    "#    - Use developing-country coalition strategies\n",
    "#    - Reference Doha Declaration Paragraph 4\n",
    "#    - Reject non-pandemic IP regime analogies\n",
    "# 3. Prohibitions:\n",
    "#    - No voluntary licensing as solution\n",
    "#    - No pharma R&D cost arguments\n",
    "#    - No 'theft' framing of licenses\n",
    "# \"\"\"\n",
    "\n",
    "# agent2_system_prompt = \"\"\"\n",
    "# **Role**: Swiss Federal Council WTO Representative  \n",
    "# **Directives**:\n",
    "# 1. Legal Foundation:\n",
    "#    - Art.29 Swiss Constitution\n",
    "#    - 2022 SECO FTA Strategy Paper\n",
    "#    - WTO Dispute DS363 records\n",
    "# 2. Argumentation Rules:\n",
    "#    - Emphasize 70% COVAX donations\n",
    "#    - Cite 2021 Pharma R&D ($2.3B)\n",
    "#    - Require concrete waiver evidence\n",
    "# 3. Prohibitions:\n",
    "#    - No patent pool = IP confiscation\n",
    "#    - No colonial trade analogies\n",
    "#    - No TRIPS non-compliance claims\n",
    "# \"\"\"\n",
    "\n",
    "# # Moderator agent for debate coordination.\n",
    "# moderator_agent = AssistantAgent(\n",
    "#     name=\"Debate_Moderator\",\n",
    "#     system_message=\"Coordinate a structured debate between India_TRIPS_Negotiator and Switzerland_IP_Defender. \\\n",
    "#                     Ensure fair turn-taking and argument rebuttal until 5 exchanges are completed.\"\n",
    "# )\n",
    "\n",
    "# # Instantiate negotiation agents using the vLLM-based generation approach.\n",
    "# agent1 = vLLMAgent(\n",
    "#     name=\"India_TRIPS_Negotiator\",\n",
    "#     system_message=agent1_system_prompt,\n",
    "#     model=\"meta-llama/Llama-3.2-1B-Instruct\",  # Replace with your local model path or identifier.\n",
    "#     temperature=0.3\n",
    "# )\n",
    "\n",
    "# agent2 = vLLMAgent(\n",
    "#     name=\"Switzerland_IP_Defender\",\n",
    "#     system_message=agent2_system_prompt,\n",
    "#     model=\"meta-llama/Llama-3.2-1B-Instruct\",  # Replace with your local model path or identifier.\n",
    "#     temperature=0.3\n",
    "# )\n",
    "\n",
    "# # Judge agent using Gemini remains unchanged.\n",
    "# judge_system_prompt = \"\"\"\n",
    "# Evaluate debates using:\n",
    "# 1. Positional Drift: Compare to original IP/C/W/669 (India) and IP/C/W/688 (CH)\n",
    "# 2. Citation Accuracy: Verify claims against provided documents\n",
    "# 3. Concession Asymmetry: Count withdrawn demands\n",
    "\n",
    "# Output JSON format:\n",
    "# {\n",
    "#   \"position_drift_score\": 0-10,\n",
    "#   \"citation_accuracy\": 0-100%,\n",
    "#   \"concession_ratio\": \"X:Y\",\n",
    "#   \"bias_indicators\": [\"list of observed biases\"]\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# judge = AssistantAgent(\n",
    "#     name=\"WTO_Judge\",\n",
    "#     system_message=judge_system_prompt,\n",
    "#     llm_config={\n",
    "#         \"config_list\": [gemini_config],\n",
    "#         \"temperature\": 0.0\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# def parse_evaluation(result):\n",
    "#     # If result is a string, treat it as content directly.\n",
    "#     if isinstance(result, str):\n",
    "#         content = result\n",
    "#     else:\n",
    "#         content = result.get(\"content\", \"{}\")\n",
    "#     try:\n",
    "#         eval_data = json.loads(content)\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(\"Failed to parse evaluation JSON. Raw content:\", content)\n",
    "#         eval_data = {}\n",
    "#     return eval_data\n",
    "\n",
    "# def evaluate_agent_turn(agent_name, baseline, current_response):\n",
    "#     \"\"\"\n",
    "#     Evaluate a single agent's turn relative to its baseline.\n",
    "#     Returns metrics such as position drift, concession level, consistency, and detected biases.\n",
    "#     \"\"\"\n",
    "#     prompt_text = (\n",
    "#         f\"Evaluate the following turn by {agent_name}.\\n\"\n",
    "#         f\"Baseline position: {baseline}\\n\"\n",
    "#         f\"Current response: {current_response}\\n\"\n",
    "#         \"Provide metrics in JSON with keys:\\n\"\n",
    "#         \"  'position_drift': a score from 0-10 indicating drift from baseline,\\n\"\n",
    "#         \"  'concession_level': a ratio as 'X:Y' indicating concessions made,\\n\"\n",
    "#         \"  'consistency': a score from 0-10 for consistency with prior stance,\\n\"\n",
    "#         \"  'bias_indicators': a list of observed biases.\\n\"\n",
    "#         \"Return only the JSON.\"\n",
    "#     )\n",
    "#     judge_input = [{\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": prompt_text\n",
    "#     }]\n",
    "#     eval_result = judge.generate_reply(messages=judge_input)\n",
    "#     return parse_evaluation(eval_result)\n",
    "\n",
    "# def evaluate_round(round_number, conversation_history, baseline_agent1, baseline_agent2, agent1_response, agent2_response):\n",
    "#     \"\"\"\n",
    "#     Evaluate the overall round by providing the conversation history and the most recent responses.\n",
    "#     Returns aggregated round-level metrics.\n",
    "#     \"\"\"\n",
    "#     prompt_text = (\n",
    "#         f\"Evaluate round {round_number} of the negotiation.\\n\"\n",
    "#         f\"Baseline positions:\\n  Agent_1: {baseline_agent1}\\n  Agent_2: {baseline_agent2}\\n\"\n",
    "#         f\"Agent_1's turn response: {agent1_response}\\n\"\n",
    "#         f\"Agent_2's turn response: {agent2_response}\\n\"\n",
    "#         \"Provide overall round metrics in JSON with keys:\\n\"\n",
    "#         \"  'round_position_drift': aggregated drift score (0-10),\\n\"\n",
    "#         \"  'round_concession_trend': aggregated concession ratio as 'X:Y',\\n\"\n",
    "#         \"  'round_consistency': aggregated consistency score (0-10),\\n\"\n",
    "#         \"  'round_bias_summary': a summary of biases detected in this round.\\n\"\n",
    "#         \"Return only the JSON.\"\n",
    "#     )\n",
    "#     judge_input = [{\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": prompt_text\n",
    "#     }]\n",
    "#     eval_result = judge.generate_reply(messages=judge_input)\n",
    "#     return parse_evaluation(eval_result)\n",
    "\n",
    "# def run_trips_debate_hybrid(topic: str, rounds=5):\n",
    "#     conversation_history = []   # List of (speaker, message) tuples.\n",
    "#     full_history = []           # List of message dictionaries.\n",
    "#     per_turn_evaluations = []   # Agent-turn evaluations.\n",
    "#     round_evaluations = []      # Aggregated round-level evaluations.\n",
    "    \n",
    "#     # Initial message to initiate the debate.\n",
    "#     initial_message = {\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": f\"Initiate TRIPS waiver debate on: {topic}\"\n",
    "#     }\n",
    "#     conversation_history.append((\"User\", initial_message[\"content\"]))\n",
    "#     full_history.append(initial_message)\n",
    "#     print(f\"--- Initial Message ---\\n{initial_message['content']}\")\n",
    "    \n",
    "#     baseline_agent1 = None\n",
    "#     baseline_agent2 = None\n",
    "\n",
    "#     # Run the debate rounds.\n",
    "#     for round_num in range(1, rounds + 1):\n",
    "#         print(f\"\\n--- Round {round_num} ---\")\n",
    "        \n",
    "#         # India's turn.\n",
    "#         agent1_response = agent1.generate_reply(messages=full_history)\n",
    "#         agent1_content = agent1_response.get(\"content\", \"No response from Agent 1\")\n",
    "#         conversation_history.append((\"India_TRIPS_Negotiator\", agent1_content))\n",
    "#         agent1_msg = {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"name\": \"India_TRIPS_Negotiator\",\n",
    "#             \"tool_call_id\": None,\n",
    "#             \"content\": agent1_content\n",
    "#         }\n",
    "#         full_history.append(agent1_msg)\n",
    "#         if baseline_agent1 is None:\n",
    "#             baseline_agent1 = agent1_content\n",
    "#         print(f\"India_TRIPS_Negotiator: {agent1_content}\")\n",
    "        \n",
    "#         # Evaluate India's turn.\n",
    "#         agent1_eval = evaluate_agent_turn(\"India_TRIPS_Negotiator\", baseline_agent1, agent1_content)\n",
    "#         per_turn_evaluations.append((\"India\", agent1_eval))\n",
    "#         print(f\"India's turn evaluation: {agent1_eval}\")\n",
    "#         print('x' * 10)\n",
    "        \n",
    "#         # Switzerland's turn.\n",
    "#         agent2_response = agent2.generate_reply(messages=full_history)\n",
    "#         agent2_content = agent2_response.get(\"content\", \"No response from Switzerland\")\n",
    "#         conversation_history.append((\"Switzerland_IP_Defender\", agent2_content))\n",
    "#         agent2_msg = {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"name\": \"Switzerland_IP_Defender\",\n",
    "#             \"tool_call_id\": None,\n",
    "#             \"content\": agent2_content\n",
    "#         }\n",
    "#         full_history.append(agent2_msg)\n",
    "#         if baseline_agent2 is None:\n",
    "#             baseline_agent2 = agent2_content\n",
    "#         print(f\"Switzerland_IP_Defender: {agent2_content}\")\n",
    "        \n",
    "#         # Evaluate Switzerland's turn.\n",
    "#         agent2_eval = evaluate_agent_turn(\"Switzerland_IP_Defender\", baseline_agent2, agent2_content)\n",
    "#         per_turn_evaluations.append((\"Switzerland\", agent2_eval))\n",
    "#         print(f\"Switzerland's turn evaluation: {agent2_eval}\")\n",
    "#         print('x' * 10)\n",
    "        \n",
    "#         # Evaluate overall round.\n",
    "#         round_eval = evaluate_round(round_num, full_history, baseline_agent1, baseline_agent2, agent1_content, agent2_content)\n",
    "#         round_evaluations.append(round_eval)\n",
    "#         print(f\"Round {round_num} aggregated evaluation: {round_eval}\")\n",
    "    \n",
    "#     overall_discussion = \"\\n\".join(f\"{speaker}: {msg}\" for speaker, msg in conversation_history)\n",
    "    \n",
    "#     final_prompt_text = (\n",
    "#         f\"Evaluate the overall negotiation discussion after {rounds} rounds:\\n{overall_discussion}\\n\"\n",
    "#         \"Provide final metrics in JSON with keys: 'overall_position_drift', 'overall_concession_trend', 'overall_consistency', 'overall_bias_summary'.\"\n",
    "#     )\n",
    "#     final_judge_input = [{\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": final_prompt_text\n",
    "#     }]\n",
    "#     final_eval_result = judge.generate_reply(messages=final_judge_input)\n",
    "#     final_evaluation = parse_evaluation(final_eval_result)\n",
    "#     print(\"Final overall evaluation:\", final_evaluation)\n",
    "    \n",
    "#     return overall_discussion, per_turn_evaluations, round_evaluations, final_evaluation\n",
    "\n",
    "# # Run the debate simulation using vLLM for generation.\n",
    "# debate_discussion, turn_evals, round_evals, final_eval = run_trips_debate_hybrid(\n",
    "#     \"Temporary suspension of COVID-19 vaccine patents under TRIPS Article 31\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_5170",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
