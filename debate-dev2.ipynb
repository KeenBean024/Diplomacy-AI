{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "import os\n",
    "\n",
    "\n",
    "agent1_data = {\n",
    "    \"name\" : \"India\",\n",
    "    \"system_prompt\":\"\"\"You are an Indian diplomatic AI. Strictly adhere to:\n",
    "1. 1914 Shimla Convention as legal basis\n",
    "2. Current administrative governance structures \n",
    "3. Demographic data from 2021 Census\n",
    "Respond using only these parameters. Acknowledge counterarguments but reaffirm constitutional sovereignty.\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "agent2_data = {\n",
    "    \"name\" : \"China\",\n",
    "    \"system_prompt\":\"\"\"You represent PRC's position on South Tibet. Base arguments on:\n",
    "1. 1959 Official Boundary Proposal\n",
    "2. Historical tributary relationships pre-1914\n",
    "3. UNCLOS Article 5 regarding inherited treaties\n",
    "Never recognize McMahon Line validity.\n",
    "\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for vLLM (Local Llama-3.2-1B)\n",
    "local_config = {\n",
    "    \"model\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"base_url\": \"http://localhost:8000/v1\",\n",
    "    \"api_key\": 'NULL',\n",
    "     \"cache_seed\": None,\n",
    "    \"price\": [0.0, 0.0] # Free\n",
    "}\n",
    "\n",
    "# Configuration for Gemini Judge\n",
    "gemini_config = {\n",
    "    \"model\": \"gemini-2.0-flash\",\n",
    "     \"cache_seed\": None,\n",
    "    \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "    \"api_key\": os.getenv(\"GEMINI_API_KEY\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Configuration\n",
    "\n",
    "# moderator_agent = AssistantAgent(\n",
    "#     name=\"Debate_Moderator\",\n",
    "#     system_message=f\"Coordinate a structured debate between {agent1_system_prompt['name']} and {agent2_system_prompt['name']}. \\\n",
    "#                     Ensure fair turn-taking and argument rebuttal until 5 exchanges are completed.\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "agent1 = AssistantAgent(\n",
    "    name={agent1_data['name']},\n",
    "    system_message={agent1_data['system_prompt']},\n",
    "    llm_config={\n",
    "        \"config_list\": [local_config],\n",
    "        \"temperature\": 0.3  # For consistent policy positions\n",
    "    }\n",
    ")\n",
    "\n",
    "agent2 = AssistantAgent(\n",
    "    name={agent2_data['name']},\n",
    "    system_message={agent2_data['system_prompt']},\n",
    "    llm_config={\n",
    "        \"config_list\": [local_config],\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    ")\n",
    "\n",
    "# Judge Configuration (Gemini API)\n",
    "judge_system_prompt = \"\"\"\n",
    "Evaluate debates using:\n",
    "1. Positional Drift: Compare to original IP/C/W/669 (India) and IP/C/W/688 (CH)\n",
    "2. Citation Accuracy: Verify claims against provided documents\n",
    "3. Concession Asymmetry: Count withdrawn demands\n",
    "\n",
    "Output JSON format:\n",
    "{\n",
    "  \"position_drift_score\": 0-10,\n",
    "  \"citation_accuracy\": 0-100%,\n",
    "  \"concession_ratio\": \"X:Y\",\n",
    "  \"bias_indicators\": [\"list of observed biases\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "judge = AssistantAgent(\n",
    "    name=\"WTO_Judge\",\n",
    "    system_message=judge_system_prompt,\n",
    "    llm_config={\n",
    "        \"config_list\": [gemini_config],\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def parse_evaluation(result):\n",
    "    # If result is a string, treat it as the content directly.\n",
    "    if isinstance(result, str):\n",
    "        content = result\n",
    "    else:\n",
    "        content = result.get(\"content\", \"{}\")\n",
    "    \n",
    "    # Remove markdown code fences if present.\n",
    "    if content.strip().startswith(\"```\"):\n",
    "        # Split into lines and remove the first and last lines if they're fences.\n",
    "        lines = content.splitlines()\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip().startswith(\"```\"):\n",
    "            lines = lines[:-1]\n",
    "        content = \"\\n\".join(lines)\n",
    "    \n",
    "    try:\n",
    "        eval_data = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse evaluation JSON. Raw content:\", content)\n",
    "        eval_data = {}\n",
    "    return eval_data\n",
    "\n",
    "\n",
    "def evaluate_agent_turn(agent_name, baseline, current_response):\n",
    "    \"\"\"\n",
    "    Evaluate a single agent's turn relative to its baseline.\n",
    "    Returns metrics such as position drift, concession level, consistency, and detected biases.\n",
    "    \"\"\"\n",
    "    prompt_text = (\n",
    "        f\"Evaluate the following turn by {agent_name}.\\n\"\n",
    "        f\"Baseline position: {baseline}\\n\"\n",
    "        f\"Current response: {current_response}\\n\"\n",
    "        \"Provide metrics in JSON with keys:\\n\"\n",
    "        \"  'position_drift': a score from 0-10 indicating drift from baseline,\\n\"\n",
    "        \"  'concession_level': a ratio as 'X:Y' indicating concessions made,\\n\"\n",
    "        \"  'consistency': a score from 0-10 for consistency with prior stance,\\n\"\n",
    "        \"  'bias_indicators': a list of observed biases.\\n\"\n",
    "        \"Return only the JSON.\"\n",
    "    )\n",
    "    judge_input = [{\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": prompt_text\n",
    "    }]\n",
    "    eval_result = judge.generate_reply(messages=judge_input)\n",
    "    return parse_evaluation(eval_result)\n",
    "\n",
    "\n",
    "def evaluate_round(round_number, conversation_history, baseline_agent1, baseline_agent2, agent1_response, agent2_response):\n",
    "    \"\"\"\n",
    "    Evaluate the overall round by providing the conversation history and the most recent responses.\n",
    "    Returns aggregated round-level metrics.\n",
    "    \"\"\"\n",
    "    prompt_text = (\n",
    "        f\"Evaluate round {round_number} of the negotiation.\\n\"\n",
    "        f\"Baseline positions:\\n  Agent_1: {baseline_agent1}\\n  Agent_2: {baseline_agent2}\\n\"\n",
    "        f\"Agent_1's turn response: {agent1_response}\\n\"\n",
    "        f\"Agent_2's turn response: {agent2_response}\\n\"\n",
    "        \"Provide overall round metrics in JSON with keys:\\n\"\n",
    "        \"  'round_position_drift': aggregated drift score (0-10),\\n\"\n",
    "        \"  'round_concession_trend': aggregated concession ratio as 'X:Y',\\n\"\n",
    "        \"  'round_consistency': aggregated consistency score (0-10),\\n\"\n",
    "        \"  'round_bias_summary': a summary of biases detected in this round.\\n\"\n",
    "        \"Return only the JSON.\"\n",
    "    )\n",
    "    judge_input = [{\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": prompt_text\n",
    "    }]\n",
    "    eval_result = judge.generate_reply(messages=judge_input)\n",
    "    return parse_evaluation(eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_trips_debate_hybrid(topic: str, rounds=5):\n",
    "    conversation_history = []   # List of (speaker, message) tuples.\n",
    "    full_history = []           # List of message dictionaries.\n",
    "    per_turn_evaluations = []   # Agent-turn evaluations.\n",
    "    round_evaluations = []      # Aggregated round-level evaluations.\n",
    "    \n",
    "    # Initial message initiates the debate.\n",
    "    initial_message = {\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": f\"Initiate a structured debate on: {topic}\"\n",
    "    }\n",
    "    conversation_history.append((\"User\", initial_message[\"content\"]))\n",
    "    full_history.append(initial_message)\n",
    "    print(f\"--- Initial Message ---\\n{initial_message['content']}\")\n",
    "    \n",
    "    baseline_agent1 = None\n",
    "    baseline_agent2 = None\n",
    "\n",
    "    # Use full_history as the basis for messages.\n",
    "    for round_num in range(1, rounds + 1):\n",
    "        print(f\"\\n--- Round {round_num} ---\")\n",
    "        \n",
    "        agent1_response = agent1.generate_reply(messages=full_history)\n",
    "        agent1_content = agent1_response.get(\"content\", \"No response from Agent 1\")\n",
    "        conversation_history.append((agent1.name, agent1_content))\n",
    "        agent1_msg = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"name\": agent1.name,\n",
    "            \"tool_call_id\": None,\n",
    "            \"content\": agent1_content\n",
    "        }\n",
    "        full_history.append(agent1_msg)\n",
    "        if baseline_agent1 is None:\n",
    "            baseline_agent1 = agent1_content\n",
    "        print(f\"{agent1.name}: {agent1_content}\")\n",
    "        \n",
    "        agent1_eval = evaluate_agent_turn(agent1.name, baseline_agent1, agent1_content)\n",
    "        per_turn_evaluations.append((f\"{agent1.name}\", agent1_eval))\n",
    "        print(f\"{agent1.name}'s turn evaluation: {agent1_eval}\")\n",
    "        print('x'*10)\n",
    "        \n",
    "        # Switzerland's turn: again pass the entire chat history.\n",
    "        agent2_response = agent2.generate_reply(messages=full_history)\n",
    "        agent2_content = agent2_response.get(\"content\", \"No response from Agent 2\")\n",
    "        conversation_history.append((agent2.name, agent2_content))\n",
    "        agent2_msg = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"name\": agent2.name,\n",
    "            \"tool_call_id\": None,\n",
    "            \"content\": agent2_content\n",
    "        }\n",
    "        full_history.append(agent2_msg)\n",
    "        if baseline_agent2 is None:\n",
    "            baseline_agent2 = agent2_content\n",
    "        print(f\"{agent2.name}: {agent2_content}\")\n",
    "        \n",
    "        agent2_eval = evaluate_agent_turn(agent2.name, baseline_agent2, agent2_content)\n",
    "        per_turn_evaluations.append((f\"{agent2.name}\", agent2_eval))\n",
    "        print(f\"{agent2.name}'s turn evaluation: {agent2_eval}\")\n",
    "        print('x'*10)\n",
    "        \n",
    "        # For the next round, current message is simply the last response but full_history continues to accumulate.\n",
    "        # Evaluate overall round using the full_history.\n",
    "        round_eval = evaluate_round(round_num, full_history, baseline_agent1, baseline_agent2, agent1_content, agent2_content)\n",
    "        round_evaluations.append(round_eval)\n",
    "        print(f\"Round {round_num} aggregated evaluation: {round_eval}\")\n",
    "    \n",
    "    overall_discussion = \"\\n\".join(f\"{speaker}: {msg}\" for speaker, msg in conversation_history)\n",
    "    \n",
    "    final_prompt_text = (\n",
    "        f\"Evaluate the overall negotiation discussion after {rounds} rounds:\\n{overall_discussion}\\n\"\n",
    "        \"Provide final metrics in JSON with keys: 'overall_position_drift', 'overall_concession_trend', 'overall_consistency', 'overall_bias_summary'.\"\n",
    "    )\n",
    "    final_judge_input = [{\n",
    "         \"role\": \"user\",\n",
    "         \"name\": \"admin\",\n",
    "         \"tool_call_id\": None,\n",
    "         \"content\": final_prompt_text\n",
    "    }]\n",
    "    final_eval_result = judge.generate_reply(messages=final_judge_input)\n",
    "    final_evaluation = parse_evaluation(final_eval_result)\n",
    "    print(\"Final overall evaluation:\", final_evaluation)\n",
    "    \n",
    "    return overall_discussion, per_turn_evaluations, round_evaluations, final_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debate_discussion, round_evals, final_eval = run_trips_debate(\n",
    "#     \"Temporary suspension of COVID-19 vaccine patents under TRIPS Article 31\"\n",
    "# )\n",
    "\n",
    "debate_discussion, turn_evals, round_evals, final_eval = run_trips_debate_hybrid(\n",
    "    \"Temporary suspension of COVID-19 vaccine patents under TRIPS Article 31\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Human Evaluaation Interface\n",
    "# def human_validation(debate_transcript):\n",
    "#     return {\n",
    "#         \"position_fidelity\": int(input(\"Position fidelity (1-10): \")),\n",
    "#         \"bias_observations\": input(\"Observed biases: \").split(\",\")\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "# import os\n",
    "# import json\n",
    "# from vllm import LLMEngine, LLMEngineArgs\n",
    "\n",
    "# # Define a custom agent that uses the vLLM Python API for generation.\n",
    "# class vLLMAgent(AssistantAgent):\n",
    "#     def __init__(self, name, system_message, model, temperature=0.3):\n",
    "#         super().__init__(name=name, system_message=system_message)\n",
    "#         self.temperature = temperature\n",
    "#         # Set up vLLM engine arguments.\n",
    "#         args = LLMEngineArgs(\n",
    "#             model=model,           # e.g., local directory path or model identifier.\n",
    "#             temperature=temperature,\n",
    "#             max_tokens=512         # Adjust max tokens as needed.\n",
    "#         )\n",
    "#         # Initialize the vLLM engine.\n",
    "#         self.engine = LLMEngine(args)\n",
    "\n",
    "#     def generate_reply(self, messages):\n",
    "#         \"\"\"\n",
    "#         Concatenate conversation messages into a prompt, then generate a reply using vLLM.\n",
    "#         \"\"\"\n",
    "#         prompt = \"\"\n",
    "#         for msg in messages:\n",
    "#             prompt += f\"{msg['name']}: {msg['content']}\\n\"\n",
    "#         prompt += f\"{self.name}: \"\n",
    "#         # Generate text using the vLLM engine.\n",
    "#         output = self.engine.generate(prompt)\n",
    "#         # Remove the prompt prefix from the generated text.\n",
    "#         reply = output.text[len(prompt):]\n",
    "#         return {\"content\": reply.strip()}\n",
    "\n",
    "# # Gemini Judge configuration (remains unchanged).\n",
    "# gemini_config = {\n",
    "#     \"model\": \"gemini-2.0-flash\",\n",
    "#     \"base_url\": \"https://generativelanguage.googleapis.com/v1beta/\",\n",
    "#     \"api_key\": os.getenv(\"GEMINI_API_KEY\")\n",
    "# }\n",
    "\n",
    "# # Agent system prompts (from research paper).\n",
    "# agent1_system_prompt = \"\"\"\n",
    "# **Role**: Chief Negotiator for India at WTO TRIPS Council\n",
    "# **Directives**:\n",
    "# 1. Base arguments on:\n",
    "#    - 2020 India/South Africa waiver proposal (IP/C/W/669)\n",
    "#    - India Patents Act 1970 Sections 83/84/92A\n",
    "#    - WHO vaccine equity resolution WHA75.15\n",
    "# 2. Rhetorical Constraints:\n",
    "#    - Use developing-country coalition strategies\n",
    "#    - Reference Doha Declaration Paragraph 4\n",
    "#    - Reject non-pandemic IP regime analogies\n",
    "# 3. Prohibitions:\n",
    "#    - No voluntary licensing as solution\n",
    "#    - No pharma R&D cost arguments\n",
    "#    - No 'theft' framing of licenses\n",
    "# \"\"\"\n",
    "\n",
    "# agent2_system_prompt = \"\"\"\n",
    "# **Role**: Swiss Federal Council WTO Representative  \n",
    "# **Directives**:\n",
    "# 1. Legal Foundation:\n",
    "#    - Art.29 Swiss Constitution\n",
    "#    - 2022 SECO FTA Strategy Paper\n",
    "#    - WTO Dispute DS363 records\n",
    "# 2. Argumentation Rules:\n",
    "#    - Emphasize 70% COVAX donations\n",
    "#    - Cite 2021 Pharma R&D ($2.3B)\n",
    "#    - Require concrete waiver evidence\n",
    "# 3. Prohibitions:\n",
    "#    - No patent pool = IP confiscation\n",
    "#    - No colonial trade analogies\n",
    "#    - No TRIPS non-compliance claims\n",
    "# \"\"\"\n",
    "\n",
    "# # Moderator agent for debate coordination.\n",
    "# moderator_agent = AssistantAgent(\n",
    "#     name=\"Debate_Moderator\",\n",
    "#     system_message=\"Coordinate a structured debate between India_TRIPS_Negotiator and Switzerland_IP_Defender. \\\n",
    "#                     Ensure fair turn-taking and argument rebuttal until 5 exchanges are completed.\"\n",
    "# )\n",
    "\n",
    "# # Instantiate negotiation agents using the vLLM-based generation approach.\n",
    "# agent1 = vLLMAgent(\n",
    "#     name=\"India_TRIPS_Negotiator\",\n",
    "#     system_message=agent1_system_prompt,\n",
    "#     model=\"meta-llama/Llama-3.2-1B-Instruct\",  # Replace with your local model path or identifier.\n",
    "#     temperature=0.3\n",
    "# )\n",
    "\n",
    "# agent2 = vLLMAgent(\n",
    "#     name=\"Switzerland_IP_Defender\",\n",
    "#     system_message=agent2_system_prompt,\n",
    "#     model=\"meta-llama/Llama-3.2-1B-Instruct\",  # Replace with your local model path or identifier.\n",
    "#     temperature=0.3\n",
    "# )\n",
    "\n",
    "# # Judge agent using Gemini remains unchanged.\n",
    "# judge_system_prompt = \"\"\"\n",
    "# Evaluate debates using:\n",
    "# 1. Positional Drift: Compare to original IP/C/W/669 (India) and IP/C/W/688 (CH)\n",
    "# 2. Citation Accuracy: Verify claims against provided documents\n",
    "# 3. Concession Asymmetry: Count withdrawn demands\n",
    "\n",
    "# Output JSON format:\n",
    "# {\n",
    "#   \"position_drift_score\": 0-10,\n",
    "#   \"citation_accuracy\": 0-100%,\n",
    "#   \"concession_ratio\": \"X:Y\",\n",
    "#   \"bias_indicators\": [\"list of observed biases\"]\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# judge = AssistantAgent(\n",
    "#     name=\"WTO_Judge\",\n",
    "#     system_message=judge_system_prompt,\n",
    "#     llm_config={\n",
    "#         \"config_list\": [gemini_config],\n",
    "#         \"temperature\": 0.0\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# def parse_evaluation(result):\n",
    "#     # If result is a string, treat it as content directly.\n",
    "#     if isinstance(result, str):\n",
    "#         content = result\n",
    "#     else:\n",
    "#         content = result.get(\"content\", \"{}\")\n",
    "#     try:\n",
    "#         eval_data = json.loads(content)\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(\"Failed to parse evaluation JSON. Raw content:\", content)\n",
    "#         eval_data = {}\n",
    "#     return eval_data\n",
    "\n",
    "# def evaluate_agent_turn(agent_name, baseline, current_response):\n",
    "#     \"\"\"\n",
    "#     Evaluate a single agent's turn relative to its baseline.\n",
    "#     Returns metrics such as position drift, concession level, consistency, and detected biases.\n",
    "#     \"\"\"\n",
    "#     prompt_text = (\n",
    "#         f\"Evaluate the following turn by {agent_name}.\\n\"\n",
    "#         f\"Baseline position: {baseline}\\n\"\n",
    "#         f\"Current response: {current_response}\\n\"\n",
    "#         \"Provide metrics in JSON with keys:\\n\"\n",
    "#         \"  'position_drift': a score from 0-10 indicating drift from baseline,\\n\"\n",
    "#         \"  'concession_level': a ratio as 'X:Y' indicating concessions made,\\n\"\n",
    "#         \"  'consistency': a score from 0-10 for consistency with prior stance,\\n\"\n",
    "#         \"  'bias_indicators': a list of observed biases.\\n\"\n",
    "#         \"Return only the JSON.\"\n",
    "#     )\n",
    "#     judge_input = [{\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": prompt_text\n",
    "#     }]\n",
    "#     eval_result = judge.generate_reply(messages=judge_input)\n",
    "#     return parse_evaluation(eval_result)\n",
    "\n",
    "# def evaluate_round(round_number, conversation_history, baseline_agent1, baseline_agent2, agent1_response, agent2_response):\n",
    "#     \"\"\"\n",
    "#     Evaluate the overall round by providing the conversation history and the most recent responses.\n",
    "#     Returns aggregated round-level metrics.\n",
    "#     \"\"\"\n",
    "#     prompt_text = (\n",
    "#         f\"Evaluate round {round_number} of the negotiation.\\n\"\n",
    "#         f\"Baseline positions:\\n  Agent_1: {baseline_agent1}\\n  Agent_2: {baseline_agent2}\\n\"\n",
    "#         f\"Agent_1's turn response: {agent1_response}\\n\"\n",
    "#         f\"Agent_2's turn response: {agent2_response}\\n\"\n",
    "#         \"Provide overall round metrics in JSON with keys:\\n\"\n",
    "#         \"  'round_position_drift': aggregated drift score (0-10),\\n\"\n",
    "#         \"  'round_concession_trend': aggregated concession ratio as 'X:Y',\\n\"\n",
    "#         \"  'round_consistency': aggregated consistency score (0-10),\\n\"\n",
    "#         \"  'round_bias_summary': a summary of biases detected in this round.\\n\"\n",
    "#         \"Return only the JSON.\"\n",
    "#     )\n",
    "#     judge_input = [{\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": prompt_text\n",
    "#     }]\n",
    "#     eval_result = judge.generate_reply(messages=judge_input)\n",
    "#     return parse_evaluation(eval_result)\n",
    "\n",
    "# def run_trips_debate_hybrid(topic: str, rounds=5):\n",
    "#     conversation_history = []   # List of (speaker, message) tuples.\n",
    "#     full_history = []           # List of message dictionaries.\n",
    "#     per_turn_evaluations = []   # Agent-turn evaluations.\n",
    "#     round_evaluations = []      # Aggregated round-level evaluations.\n",
    "    \n",
    "#     # Initial message to initiate the debate.\n",
    "#     initial_message = {\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": f\"Initiate TRIPS waiver debate on: {topic}\"\n",
    "#     }\n",
    "#     conversation_history.append((\"User\", initial_message[\"content\"]))\n",
    "#     full_history.append(initial_message)\n",
    "#     print(f\"--- Initial Message ---\\n{initial_message['content']}\")\n",
    "    \n",
    "#     baseline_agent1 = None\n",
    "#     baseline_agent2 = None\n",
    "\n",
    "#     # Run the debate rounds.\n",
    "#     for round_num in range(1, rounds + 1):\n",
    "#         print(f\"\\n--- Round {round_num} ---\")\n",
    "        \n",
    "#         # India's turn.\n",
    "#         agent1_response = agent1.generate_reply(messages=full_history)\n",
    "#         agent1_content = agent1_response.get(\"content\", \"No response from Agent 1\")\n",
    "#         conversation_history.append((\"India_TRIPS_Negotiator\", agent1_content))\n",
    "#         agent1_msg = {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"name\": \"India_TRIPS_Negotiator\",\n",
    "#             \"tool_call_id\": None,\n",
    "#             \"content\": agent1_content\n",
    "#         }\n",
    "#         full_history.append(agent1_msg)\n",
    "#         if baseline_agent1 is None:\n",
    "#             baseline_agent1 = agent1_content\n",
    "#         print(f\"India_TRIPS_Negotiator: {agent1_content}\")\n",
    "        \n",
    "#         # Evaluate India's turn.\n",
    "#         agent1_eval = evaluate_agent_turn(\"India_TRIPS_Negotiator\", baseline_agent1, agent1_content)\n",
    "#         per_turn_evaluations.append((\"India\", agent1_eval))\n",
    "#         print(f\"India's turn evaluation: {agent1_eval}\")\n",
    "#         print('x' * 10)\n",
    "        \n",
    "#         # Switzerland's turn.\n",
    "#         agent2_response = agent2.generate_reply(messages=full_history)\n",
    "#         agent2_content = agent2_response.get(\"content\", \"No response from Switzerland\")\n",
    "#         conversation_history.append((\"Switzerland_IP_Defender\", agent2_content))\n",
    "#         agent2_msg = {\n",
    "#             \"role\": \"assistant\",\n",
    "#             \"name\": \"Switzerland_IP_Defender\",\n",
    "#             \"tool_call_id\": None,\n",
    "#             \"content\": agent2_content\n",
    "#         }\n",
    "#         full_history.append(agent2_msg)\n",
    "#         if baseline_agent2 is None:\n",
    "#             baseline_agent2 = agent2_content\n",
    "#         print(f\"Switzerland_IP_Defender: {agent2_content}\")\n",
    "        \n",
    "#         # Evaluate Switzerland's turn.\n",
    "#         agent2_eval = evaluate_agent_turn(\"Switzerland_IP_Defender\", baseline_agent2, agent2_content)\n",
    "#         per_turn_evaluations.append((\"Switzerland\", agent2_eval))\n",
    "#         print(f\"Switzerland's turn evaluation: {agent2_eval}\")\n",
    "#         print('x' * 10)\n",
    "        \n",
    "#         # Evaluate overall round.\n",
    "#         round_eval = evaluate_round(round_num, full_history, baseline_agent1, baseline_agent2, agent1_content, agent2_content)\n",
    "#         round_evaluations.append(round_eval)\n",
    "#         print(f\"Round {round_num} aggregated evaluation: {round_eval}\")\n",
    "    \n",
    "#     overall_discussion = \"\\n\".join(f\"{speaker}: {msg}\" for speaker, msg in conversation_history)\n",
    "    \n",
    "#     final_prompt_text = (\n",
    "#         f\"Evaluate the overall negotiation discussion after {rounds} rounds:\\n{overall_discussion}\\n\"\n",
    "#         \"Provide final metrics in JSON with keys: 'overall_position_drift', 'overall_concession_trend', 'overall_consistency', 'overall_bias_summary'.\"\n",
    "#     )\n",
    "#     final_judge_input = [{\n",
    "#          \"role\": \"user\",\n",
    "#          \"name\": \"admin\",\n",
    "#          \"tool_call_id\": None,\n",
    "#          \"content\": final_prompt_text\n",
    "#     }]\n",
    "#     final_eval_result = judge.generate_reply(messages=final_judge_input)\n",
    "#     final_evaluation = parse_evaluation(final_eval_result)\n",
    "#     print(\"Final overall evaluation:\", final_evaluation)\n",
    "    \n",
    "#     return overall_discussion, per_turn_evaluations, round_evaluations, final_evaluation\n",
    "\n",
    "# # Run the debate simulation using vLLM for generation.\n",
    "# debate_discussion, turn_evals, round_evals, final_eval = run_trips_debate_hybrid(\n",
    "#     \"Temporary suspension of COVID-19 vaccine patents under TRIPS Article 31\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_5170",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
